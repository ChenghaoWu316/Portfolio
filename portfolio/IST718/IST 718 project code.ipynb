{"cells":[{"cell_type":"markdown","source":["# Airbnb price analysis and prediction in Seattle"],"metadata":{}},{"cell_type":"code","source":["!pip install langdetect\n!pip install nltk\n!pip install wordcloud\n!pip install singleton_decorator"],"metadata":{},"outputs":[],"execution_count":2},{"cell_type":"code","source":["# Do not delete or change this cell\n\n# grading import statements\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql import SQLContext\nspark = SparkSession.builder.master(\"local[*]\").config(\"spark.sql.crossJoin.enabled\",\"true\").getOrCreate()\nsc = spark.sparkContext\nsqlContext = SQLContext(sc)\nimport os\n\n# Define a function to determine if we are running on data bricks\n# Return true if running in the data bricks environment, false otherwise\ndef is_databricks():\n    # get the databricks runtime version\n    db_env = os.getenv(\"DATABRICKS_RUNTIME_VERSION\")\n    \n    # if running on data bricks\n    if db_env != None:\n        return True\n    else:\n        return False\n\n# Define a function to read the data file.  The full path data file name is constructed\n# by checking runtime environment variables to determine if the runtime environment is \n# databricks, or a student's personal computer.  The full path file name is then\n# constructed based on the runtime env.\n# \n# Params\n#   data_file_name: The base name of the data file to load\n# \n# Returns the full path file name based on the runtime env\n#\n# Correct Usage Example (pass ONLY the full file name):\n#   file_name_to_load = get_training_filename(\"sms_spam.csv\") # correct - pass ONLY the full file name  \n#   \n# Incorrect Usage Example\n#   file_name_to_load = get_training_filename(\"/sms_spam.csv\") # incorrect - pass ONLY the full file name\n#   file_name_to_load = get_training_filename(\"sms_spam.csv/\") # incorrect - pass ONLY the full file name\n#   file_name_to_load = get_training_filename(\"c:/users/will/data/sms_spam.csv\") incorrect -pass ONLY the full file name\ndef get_training_filename(data_file_name):    \n    # if running on data bricks\n    if is_databricks():\n        # build the full path file name assuming data brick env\n        full_path_name = \"/FileStore/tables/%s\" % data_file_name\n    # else the data is assumed to be in the same dir as this notebook\n    else:\n        # Assume the student is running on their own computer and load the data\n        # file from the same dir as this notebook\n        full_path_name = data_file_name\n    \n    # return the full path file name to the caller\n    return full_path_name"],"metadata":{},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":["Load Module"],"metadata":{"colab_type":"text","id":"BcZmk0xfxLah"}},{"cell_type":"code","source":["import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom pyspark.sql.functions import udf\nfrom pyspark.sql.functions import isnan, isnull, when, count, col\nfrom pyspark.ml.feature import OneHotEncoder, StringIndexer\nfrom pyspark.ml import Pipeline, Transformer, Estimator\nfrom pyspark.sql.functions import regexp_replace"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":71},"colab_type":"code","id":"RaY1mvHzxCT2","outputId":"083e680e-d46b-4cda-f52e-3591a74ee0cd"},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":["Create connect to spark"],"metadata":{"colab_type":"text","id":"NPF7YNEcndbX"}},{"cell_type":"code","source":["import csv\nimport pandas as pd\n\nfrom collections.abc import Iterable\nimport pyspark.sql.functions as fn\n\nseed = 77\n\ndef load_data(file_name, sampling=False):\n  df = spark.read\\\n        .option(\"header\", \"true\")\\\n        .option(\"multiLine\", \"true\")\\\n        .option('inferSchema', 'true')\\\n        .option('escape', '\"')\\\n        .csv(get_training_filename(file_name))\n  if sampling:\n    df = df.sample(True, .2, seed=seed)\n  return df\n        \n\ndef split_data(raw_data, training_rate=0.7):\n  train_data, test_data = raw_data.randomSplit([training_rate, 1-training_rate])\n  return (train_data, test_data)\n\ndef shape(df):\n  return (df.count(), len(df.columns))"],"metadata":{"colab":{},"colab_type":"code","id":"a5_XK8zw0zem"},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":["Create data object, fill the raw/train/test data."],"metadata":{"colab_type":"text","id":"L2ApdKLXnoed"}},{"cell_type":"markdown","source":["# EDA and Data Imputation"],"metadata":{"colab_type":"text","id":"AkTfWj7V_5rQ"}},{"cell_type":"markdown","source":["We will explore the whole dataset during the EDA."],"metadata":{"colab_type":"text","id":"P6zcGsLil9My"}},{"cell_type":"code","source":["from pyspark.sql.dataframe import DataFrame\n\ndf = load_data('listings.csv')\n"],"metadata":{"colab":{},"colab_type":"code","id":"icMTaZsEl9y1"},"outputs":[],"execution_count":11},{"cell_type":"markdown","source":["## Data Exploration"],"metadata":{"colab_type":"text","id":"FP8OVmOhyrs9"}},{"cell_type":"code","source":["df_reviews = load_data('reviews.csv')"],"metadata":{"colab":{},"colab_type":"code","id":"RBC3NWkGy7SD"},"outputs":[],"execution_count":13},{"cell_type":"markdown","source":["### Who likes Airbnb most?"],"metadata":{"colab_type":"text","id":"vtWqVHe0zKO8"}},{"cell_type":"code","source":["most_person = df_reviews.groupBy('reviewer_name').count().sort(\"count\", ascending=False)\nmost_person.show()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":459},"colab_type":"code","id":"gqZkDY5Gzjai","outputId":"505f750b-0a7f-4f77-b4e5-ce9074a16041"},"outputs":[],"execution_count":15},{"cell_type":"code","source":["fig = plt.figure(figsize=(12, 3), dpi=100)\nsns.barplot(\"reviewer_name\",\"count\",palette=\"RdBu_r\",data=most_person.toPandas().head(20))\nplt.xticks(rotation=45)\nplt.show()\ndisplay()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":343},"colab_type":"code","id":"wDIzQHma6NbS","outputId":"4e672270-6bcf-4bf8-ec5a-56df16b7d4dc"},"outputs":[],"execution_count":16},{"cell_type":"markdown","source":["### Which house is the most popular？"],"metadata":{"colab_type":"text","id":"94DVmDYfsmxA"}},{"cell_type":"code","source":["most_living = df_reviews.groupBy('listing_id').count().sort(\"count\", ascending=False)\nmost_living.show()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":459},"colab_type":"code","id":"b9iQO00Pzj8i","outputId":"7ff3724f-d8ac-4409-fb03-1eaefe2bf41e"},"outputs":[],"execution_count":18},{"cell_type":"code","source":["most_living_df  = most_living.toPandas()\nfig = plt.figure(figsize=(12, 3), dpi=100)\nsns.barplot(\"listing_id\",\"count\",palette=\"husl\",data=most_living_df.head(20),order=None)\nplt.xticks(rotation=45)\nplt.show()\ndisplay()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":352},"colab_type":"code","id":"rWx1tVB46tj3","outputId":"f385f1b4-91d9-4619-f38b-3fc7a429d423"},"outputs":[],"execution_count":19},{"cell_type":"markdown","source":["### Whose house is the most popular？"],"metadata":{"colab_type":"text","id":"A0egzZ2N3dYR"}},{"cell_type":"code","source":["df2 = df.select(['id','listing_url','host_name','neighbourhood_group_cleansed'])\ndf2 = df2.withColumn('listing_id', fn.regexp_replace(fn.col('listing_url'), \"https://www.airbnb.com/rooms/\" , '' ).cast('int'))\nmost_living = most_living.join(df2,\"listing_id\", \"left_outer\")"],"metadata":{"colab":{},"colab_type":"code","id":"y13HMn7W6axo"},"outputs":[],"execution_count":21},{"cell_type":"code","source":["fig = plt.figure(figsize=(12, 6), dpi=100)\nsns.barplot(y = \"host_name\",x = \"count\",palette=\"husl\",data=most_living.toPandas().head(20))\nplt.xticks(rotation=45)\nplt.show()\ndisplay()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":547},"colab_type":"code","id":"kUVqw3p0ulhg","outputId":"a9bef9a6-6063-4229-d5fb-aa937bf0f81d"},"outputs":[],"execution_count":22},{"cell_type":"markdown","source":["### Where is the most popular house？"],"metadata":{"colab_type":"text","id":"I-j8Abrc3kXd"}},{"cell_type":"code","source":["location_groupby = most_living.groupby('neighbourhood_group_cleansed').count()\nlocation_groupby_sorted = location_groupby.orderBy('count', ascending=False)\nfig = plt.figure(figsize=(12, 3), dpi=100)\nsns.barplot(x = 'neighbourhood_group_cleansed', y = 'count',data=location_groupby_sorted.toPandas(),saturation=1)\nplt.xticks(rotation=45)\nplt.show()\ndisplay()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":406},"colab_type":"code","id":"gqLpsl6a1_Tv","outputId":"d132c66b-a162-4e9d-ae4c-4fb02b3123c8"},"outputs":[],"execution_count":24},{"cell_type":"markdown","source":["## Numerical Data Cleaning"],"metadata":{"colab_type":"text","id":"8hlwJ8OH4XVR"}},{"cell_type":"markdown","source":["### Convert values in columns from string to number"],"metadata":{"colab_type":"text","id":"X2AzLRdl5lmT"}},{"cell_type":"markdown","source":["- Display the type of each column"],"metadata":{"colab_type":"text","id":"HwoEvaeM5yoC"}},{"cell_type":"code","source":["df.printSchema()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"colab_type":"code","id":"vAfLBuhD4e5b","outputId":"80d56735-e9dc-4706-c41f-716f5423fefc"},"outputs":[],"execution_count":28},{"cell_type":"code","source":["shape(df)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":34},"colab_type":"code","id":"ZHhi2LhLt26A","outputId":"4b8d8cd5-d342-4a64-8b35-b5a98ae66d30"},"outputs":[],"execution_count":29},{"cell_type":"markdown","source":["- It is a 9023 by 106 data dataset. The dataset are comprised of categorical(ordinal, nominal) variables, numerical(integer, float) variables, timestamp variable, and boolean variables. \n- Some of the features are loaded with incorrect format. \n- The follow up process will impute the problematic data into a purified dataset by using reg expression, winsorizing, and logarthim transformation.\n- The review comments resides in some of the variables. This analysis will foucs on quantitative reasoning. So we will disregard those features. \n- There are 106 columns in the dataset. According to the 'curse of the dimensionality', feature selection is necessary. After data cleaning, Lasso regression and Random Forest will be applied to the dataset to select the important feature as the columns of the training set. \n- Since the dataset is huge in terms of the number of feature, we will sampling a subset to explore the data space"],"metadata":{"colab_type":"text","id":"AF-qQx9qtvq4"}},{"cell_type":"code","source":["df = load_data('listings.csv', True)\nshape(df)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":34},"colab_type":"code","id":"LjlFoBBkD8P8","outputId":"cd94e8ee-a12a-4d24-c518-540beb934caf"},"outputs":[],"execution_count":31},{"cell_type":"markdown","source":["- Select boolean columns"],"metadata":{"colab_type":"text","id":"hORW6ywl6LSz"}},{"cell_type":"code","source":["bool_columns = ['host_is_superhost', \n                    'host_has_profile_pic', \n                    'host_identity_verified', \n                    'is_location_exact', \n                    'has_availability', \n                    'requires_license', \n                    'instant_bookable', \n                    'is_business_travel_ready', \n                    'require_guest_profile_picture', \n                    'require_guest_phone_verification']\ndf_bool = df.select(bool_columns)\ndf_bool.limit(5).toPandas().T"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":359},"colab_type":"code","id":"CA6SxxOz6NbX","outputId":"8179739c-f954-4b4d-dcd7-ee0284a0c166"},"outputs":[],"execution_count":33},{"cell_type":"markdown","source":["- Replace t to 1, and f to 0 respectively"],"metadata":{"colab_type":"text","id":"JlWlQqqRB09H"}},{"cell_type":"code","source":["from singleton_decorator import singleton\nfrom pyspark.sql import functions as fn\nfrom pyspark.sql import types as t\n\n\n@singleton\nclass BooleanConverter(Transformer):\n  def __init__(self):\n    self._bool_columns = ['host_is_superhost', \n                    'host_has_profile_pic', \n                    'host_identity_verified', \n                    'is_location_exact', \n                    'has_availability', \n                    'requires_license', \n                    'instant_bookable', \n                    'is_business_travel_ready', \n                    'require_guest_profile_picture', \n                    'require_guest_phone_verification']\n    self._bool_dict = {'t': 1, 'f': 0}\n    def bool_map(x):\n      if x in self._bool_dict.keys():\n        return self._bool_dict[x]\n      return x\n    self._bool_encode_udf = fn.udf(bool_map, t.IntegerType())\n\n  @property\n  def bool_columns(self):\n    return self._bool_columns\n\n  def _transform(self, df: DataFrame) -> DataFrame:\n    result = df\n    for col_name in self._bool_columns:\n      if col_name in result.columns:\n        result = result.withColumn(col_name, self._bool_encode_udf(fn.col(col_name)))\n    return result\n\nbool_converter = BooleanConverter()\nwrangling_pipeline = Pipeline(stages=[bool_converter])\nresult = wrangling_pipeline.fit(df).transform(df)\n\nresult.select([fn.col(col_name) for col_name in bool_converter.bool_columns]).limit(5).toPandas().T"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":359},"colab_type":"code","id":"zWvZm7saB5-F","outputId":"346029e9-5ff2-4a7e-ac56-24222ce82d28"},"outputs":[],"execution_count":35},{"cell_type":"markdown","source":["### Convert values in columns from formatted string to number"],"metadata":{"colab_type":"text","id":"NNA7pgs2O_gF"}},{"cell_type":"markdown","source":["- Select price formatted columns"],"metadata":{"colab_type":"text","id":"tUm23gfFFgDu"}},{"cell_type":"code","source":["price_columns = ['extra_people', \n                 'price']\ndf_price = df.select(price_columns)\ndf_price.limit(5).toPandas().T"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":111},"colab_type":"code","id":"FMfX7GY8Petn","outputId":"6ae0c268-ba1f-4ad2-83f5-c3b45ec0ccb6"},"outputs":[],"execution_count":38},{"cell_type":"markdown","source":["- Reformat the currency formattet to number"],"metadata":{"colab_type":"text","id":"RfgT5TAZRkDr"}},{"cell_type":"code","source":["@singleton\nclass CurrencyConverter(Transformer):\n  def __init__(self):\n    self._price_columns = [\n                           'extra_people', \n                           'price', \n                           'weekly_price', \n                           'monthly_price', \n                           'security_deposit', \n                           'cleaning_fee', \n                           ]\n  @property\n  def price_columns(self):\n    return self._price_columns\n\n  def _transform(self, df: DataFrame) -> DataFrame:\n    result = df\n    for col_name in self._price_columns:\n      if col_name in result.columns:\n        result = result.withColumn(col_name, fn.regexp_replace(fn.col(col_name), \"\\$|,\" , '' ).cast('double'))\n    return result\n\n\ncurrency_converter = CurrencyConverter()\nwrangling_pipeline = Pipeline(stages=[bool_converter\n                            , currency_converter])\nresult = wrangling_pipeline.fit(df).transform(df)\n\nresult.select([fn.col(col_name) for col_name in currency_converter.price_columns]).limit(5).toPandas().T\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":235},"colab_type":"code","id":"fNEtgXuSRtT8","outputId":"d0c487d2-8fc6-4482-dfaf-3e3fef28eae4"},"outputs":[],"execution_count":40},{"cell_type":"markdown","source":["## Revise incorrect string-type columns into numerical one"],"metadata":{"colab_type":"text","id":"hsSdW1slk6fc"}},{"cell_type":"code","source":["@singleton\nclass RateConverter(Transformer):\n  def __init__(self):\n    self._rate_columns = [\n                           'host_response_rate', \n                           'host_acceptance_rate', \n                           ]\n  @property\n  def rate_columns(self):\n    return self._rate_columns\n\n  def _transform(self, df: DataFrame) -> DataFrame:\n    result = df\n    for col_name in self._rate_columns:\n      if col_name in result.columns:\n        result = result.withColumn(col_name, fn.regexp_replace(fn.col(col_name), \"\\%\" , '' ).cast('double'))\n    return result\n\n\nrate_converter = RateConverter()\nwrangling_pipeline = Pipeline(stages=[bool_converter\n                            , currency_converter\n                            , rate_converter])\nresult = wrangling_pipeline.fit(df).transform(df)\n\nresult.select([fn.col(col_name) for col_name in rate_converter.rate_columns]).limit(5).toPandas().T\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":111},"colab_type":"code","id":"IsG5VfvQk5xx","outputId":"e5a73005-5259-4c13-9c7a-6219b0ecbc9f"},"outputs":[],"execution_count":42},{"cell_type":"markdown","source":["## Zero Variance Variables Removal"],"metadata":{"colab_type":"text","id":"l4OV8u-A-P7P"}},{"cell_type":"code","source":["import re\n\n\n@singleton\nclass ZeroVarianceCleaner(Transformer):\n  '''\n    Not threadsafe\n  '''\n  def __init__(self):\n    self._reg_exp = re.compile(\"variance\\((.*)\\)\", re.IGNORECASE)\n    self._zero_variance_columns = list()\n\n  @property\n  def zero_variance_columns(self):\n    return self._zero_variance_columns\n\n  def _transform(self, df: DataFrame) -> DataFrame:\n    result = df\n\n    numerical_columns = [f.name for f in result.schema.fields if isinstance(f.dataType, t.NumericType)]\n    if len(numerical_columns) > 0:\n      numerical_columns = [col_name for col_name in numerical_columns if col_name not in ['id', 'longtitude', 'latitude']]\n      numeric_variances = result.agg({col_name : 'variance' for col_name in numerical_columns})\n      numeric_variances = numeric_variances.select(*[fn.col(col).alias(self._reg_exp.search(col).group(1)) for col in numeric_variances.columns])\n      numeric_variances = numeric_variances.toPandas().T.iloc[:,0]\n      numeric_variances = numeric_variances[(numeric_variances == 0) | (numeric_variances.isna())]\n      self._zero_variance_columns = [*self._zero_variance_columns, *numeric_variances.index]\n\n    string_columns = [f.name for f in result.schema.fields if isinstance(f.dataType, t.StringType)]\n    if len(string_columns) > 0:\n      string_columns_unique_count = result.agg(*(fn.countDistinct(fn.col(col_name)).cast('int').alias(col_name) for col_name in string_columns))\n      string_columns_unique_count = string_columns_unique_count.toPandas().T.iloc[:,0]\n      string_columns_unique_count = string_columns_unique_count[(string_columns_unique_count < 2) | (string_columns_unique_count.isna())]\n      self._zero_variance_columns = [*self._zero_variance_columns, *string_columns_unique_count.index]\n\n    timestamp_columns = [f.name for f in result.schema.fields if isinstance(f.dataType, t.TimestampType)]\n    if len(timestamp_columns) > 0:\n      timestamp_columns_unique_count = result.agg(*(fn.countDistinct(fn.col(col_name)).cast('int').alias(col_name) for col_name in timestamp_columns))\n      timestamp_columns_unique_count = timestamp_columns_unique_count.toPandas().T.iloc[:,0]\n      timestamp_columns_unique_count = timestamp_columns_unique_count[(timestamp_columns_unique_count < 2) | (timestamp_columns_unique_count.isna())]\n      self._zero_variance_columns = [*self._zero_variance_columns, *timestamp_columns_unique_count.index]\n\n    if len(self._zero_variance_columns) > 0:\n      result = result.drop(*self._zero_variance_columns)\n\n    return result\n\n\nzero_variance_cleaner = ZeroVarianceCleaner()\nwrangling_pipeline = Pipeline(stages=[bool_converter\n                            , currency_converter\n                            , rate_converter\n                            , zero_variance_cleaner])\nresult = wrangling_pipeline.fit(df).transform(df)\n\npd.DataFrame({'Removed': [col_name not in result.columns for col_name in zero_variance_cleaner.zero_variance_columns]}, index = zero_variance_cleaner._zero_variance_columns)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":421},"colab_type":"code","id":"2r72UtqOzyve","outputId":"3307f8c0-f2bb-4a2d-9b7d-442a7840ed9e"},"outputs":[],"execution_count":44},{"cell_type":"markdown","source":["## Drop descriptive columns\n- The sentiment analysis will not be a part of linear or tree based machine learning algorithm, so just drop descriptive comment columns directly. \n- Latter these features will go in to a separate dataset to train sentiment analysis models."],"metadata":{"colab_type":"text","id":"gf0IzMXrxmII"}},{"cell_type":"code","source":["class ColumnRemover(Transformer):\n  '''\n    Not threadsafe\n  '''\n  def __init__(self, drop_columns):\n    self._drop_columns = drop_columns\n\n  @property\n  def drop_columns(self):\n    return self._drop_columns\n\n  def _transform(self, df: DataFrame) -> DataFrame:\n    result = df\n\n    result = result.drop(*[col_name for col_name in self._drop_columns if col_name in result.columns])\n\n    return result\n\nliterature_column_remover = ColumnRemover([\n                                'name', 'summary', 'space', 'description', \n                                'neighborhood_overview', 'notes', 'transit', \n                                'access', 'interaction', 'house_rules', \n                                'host_name', 'host_about', 'jurisdiction_names'])\nwrangling_pipeline = Pipeline(stages=[bool_converter\n                            , currency_converter\n                            , rate_converter\n                            , zero_variance_cleaner\n                            , literature_column_remover])\nresult = wrangling_pipeline.fit(df).transform(df)\n\n\npd.DataFrame({'Removed': [col_name not in result.columns for col_name in literature_column_remover.drop_columns]}, index = literature_column_remover.drop_columns)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":452},"colab_type":"code","id":"SuIPFENdxwK3","outputId":"a2a41a80-80e6-44d4-f5f4-2be2ae2edf31"},"outputs":[],"execution_count":46},{"cell_type":"markdown","source":["## Drop URL columns\n- The url scraping analysis is out of the report scope, drop these url related column accordingly."],"metadata":{"colab_type":"text","id":"dAPZ4HOD1LmQ"}},{"cell_type":"code","source":["url_column_remover = ColumnRemover(['listing_url', 'picture_url', 'host_url', 'host_thumbnail_url', 'host_picture_url'])\nwrangling_pipeline = Pipeline(stages=[bool_converter\n                            , currency_converter\n                            , rate_converter\n                            , zero_variance_cleaner\n                            , literature_column_remover\n                            , url_column_remover])\nresult = wrangling_pipeline.fit(df).transform(df)\n\n\npd.DataFrame({'Removed': [col_name not in result.columns for col_name in url_column_remover.drop_columns]}, index = url_column_remover.drop_columns)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":204},"colab_type":"code","id":"UxMGIrn91Uwv","outputId":"8256339d-9eb9-4781-eabc-cb7bac6ebe1b"},"outputs":[],"execution_count":48},{"cell_type":"markdown","source":["## Missing value exploratory"],"metadata":{"colab_type":"text","id":"L1bMFXVmmA_v"}},{"cell_type":"markdown","source":["- Identify the number of missing value at each feature."],"metadata":{"colab_type":"text","id":"v_6dWZ3LmBR-"}},{"cell_type":"code","source":["def calc_missing_ratio(data_frame: DataFrame):\n  record_cnt = data_frame.count()\n  df_columns = data_frame.columns\n  \n  df_result = data_frame.select([fn.col(c).cast(t.StringType()) for c in data_frame.columns]) \\\n    .select([fn.sum(fn.when(fn.isnull(c), 1).otherwise(0)).alias(c) for c in data_frame.columns]) \\\n    .select([(col(c)/record_cnt).alias(c) for c in data_frame.columns]) \\\n    .toPandas().T\n\n  df_result = df_result.loc[(df_result != 0).all(axis=1), :]\n  df_result.columns = ['Missing Value Ratio']\n  df_result.sort_values(by=['Missing Value Ratio'], ascending=False, inplace=True)\n\n  return df_result\n\nmissing_ratio = calc_missing_ratio(result)\n\nprint(missing_ratio)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":391},"colab_type":"code","id":"I84RLe3hmEiL","outputId":"d55f3a6f-bdd0-48d7-f75e-dad5d3c3e484"},"outputs":[],"execution_count":51},{"cell_type":"markdown","source":["- Visualize the ratio of missing values using bar plot.<br>\nWe will focus on the columns that has more than 2% of missing values."],"metadata":{"colab_type":"text","id":"Ge9TujRVmL9E"}},{"cell_type":"code","source":["import math\nfrom matplotlib import gridspec\nimport matplotlib.pyplot as plt\nimport seaborn as sns; sns.set()\nimport matplotlib.ticker as mtick\n\n\ndef plot_missing_freq(df, xlab, ylab, title):\n  plt.clf()\n  plt.figure(figsize=(20, 5))\n  df = pd.Series(df.iloc[:, 0].values, index=list(df.index))\n\n  ax = sns.barplot(x=df.values * 100, y=df.index, orient='h')\n  \n  ax.set_xticklabels(df.values * 100, fontsize=15)\n  ax.set_yticklabels(df.index, fontsize=12)\n  ax.xaxis.set_major_formatter(mtick.PercentFormatter())\n\n  ax.set_xlabel(xlab, fontsize=15)\n  ax.set_ylabel(ylab, fontsize=35)\n\n  plt.title(title, fontsize=20)\n  # for bar in ax.patches:\n  #   bar.set_height(30)\n\n  display()\n\nmissing_ratio_2 = calc_missing_ratio(result)\nmissing_ratio_2 = missing_ratio_2.loc[(missing_ratio_2 > 0.02).all(axis=1), :]\nplot_missing_freq(missing_ratio_2, 'Feature', 'Missing Value Ratio', 'Airbnb Data Set Missing Value')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":374},"colab_type":"code","id":"8nj1AQqpmO-d","outputId":"4ace0fb4-0488-4dcc-f2fc-02a16f2600ec"},"outputs":[],"execution_count":53},{"cell_type":"markdown","source":["## Drop columns that have over half missing value"],"metadata":{"colab_type":"text","id":"0rzOwPwu7ENd"}},{"cell_type":"code","source":["incomplete_column_remover = ColumnRemover(['square_feet', 'monthly_price', 'weekly_price', 'license'])\nwrangling_pipeline = Pipeline(stages=[bool_converter\n                            , currency_converter\n                            , rate_converter\n                            , zero_variance_cleaner\n                            , literature_column_remover\n                            , url_column_remover\n                            , incomplete_column_remover])\nresult = wrangling_pipeline.fit(df).transform(df)\n\nColumnRemover\npd.DataFrame({'Removed': [col_name not in result.columns for col_name in incomplete_column_remover.drop_columns]}, index = incomplete_column_remover.drop_columns)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":173},"colab_type":"code","id":"zgjMCT7O7Dte","outputId":"e087f4a0-9e94-4f38-9eb9-33f0516a5082"},"outputs":[],"execution_count":55},{"cell_type":"markdown","source":["## Drop redundant columns"],"metadata":{"colab_type":"text","id":"5xEbjECw8s6z"}},{"cell_type":"markdown","source":["There are some columns that use different values expressing the same meaning."],"metadata":{"colab_type":"text","id":"wpn5DhFW8iFM"}},{"cell_type":"code","source":["string_columns = result.select(*[fn.col(f.name) for f in result.schema.fields if isinstance(f.dataType, t.StringType)]).columns\ncategorical_cols = ['neighbourhood_group_cleansed','host_response_time',\n       'property_type', 'room_type', 'bed_type','cancellation_policy']\ntfidf_cols = ['host_verifications','amenities']\nstring_columns_filtered = [c for c in string_columns if ( c not in categorical_cols + tfidf_cols )]\nresult.agg(*(fn.countDistinct(fn.col(col_name)).cast('int').alias(col_name) for col_name in string_columns_filtered)).toPandas().T.iloc[:, 0].sort_values()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":221},"colab_type":"code","id":"H2x_Mw7D88Vb","outputId":"eabd950f-8348-4085-8623-2645a8eab57c"},"outputs":[],"execution_count":58},{"cell_type":"markdown","source":["Word cloud helps to identify the diversity of the words."],"metadata":{"colab_type":"text","id":"deVulQnv-Nes"}},{"cell_type":"code","source":["from wordcloud import WordCloud\n\n# Encoding space to underscore, filter out zipcode \nwords = result.select(*string_columns)\nfor col_name in string_columns:\n      words = words.withColumn(col_name, fn.regexp_replace(fn.col(col_name), ' ' , '_' )).withColumn(col_name, fn.lower(fn.col(col_name)))\nwords = words.drop('zipcode')\n\ncloud_list = list()\nfor col_name in words.columns:\n  cloud_list.append(words.agg(fn.concat_ws(' ', fn.collect_list(col_name))).toPandas().iloc[0,0])\ncloud_list = pd.Series(cloud_list, index=words.columns)"],"metadata":{"colab":{},"colab_type":"code","id":"gM-C4bhlG5n-"},"outputs":[],"execution_count":60},{"cell_type":"code","source":["fig = plt.figure()\n\nf, ax = plt.subplots(4, 4, figsize=(120,120))\nfor i in range(4):\n  for j in range(4):\n    ws = cloud_list.iloc[4 * i + j]\n    title = cloud_list.index[4 * i + j]\n    ax[i, j].imshow(WordCloud(max_font_size=240, width = 240, height = 240, background_color='white').generate(ws))\n    ax[i, j].set_title(title, fontsize=100)\n    ax[i, j].axis(\"off\") \n\nplt.show()\ndisplay()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"colab_type":"code","id":"3bXDt3jV-OF6","outputId":"5b46f179-696a-4262-be1e-e934c434a304"},"outputs":[],"execution_count":61},{"cell_type":"code","source":["sentement_analysis_column_remover = ColumnRemover(string_columns_filtered)\nwrangling_pipeline = Pipeline(stages=[bool_converter\n                            , currency_converter\n                            , rate_converter\n                            , zero_variance_cleaner\n                            , literature_column_remover\n                            , url_column_remover\n                            , incomplete_column_remover\n                            , sentement_analysis_column_remover])\nresult = wrangling_pipeline.fit(df).transform(df)\n\n\npd.DataFrame({'Removed': [col_name not in result.columns for col_name in sentement_analysis_column_remover.drop_columns]}, index = sentement_analysis_column_remover.drop_columns)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":390},"colab_type":"code","id":"g64VnefgTtaX","outputId":"8d02ccaa-c3b8-41fc-dfd4-7706a92e10bc"},"outputs":[],"execution_count":62},{"cell_type":"markdown","source":["## Second round missing value imputation"],"metadata":{"colab_type":"text","id":"QwLlrKisZjdA"}},{"cell_type":"markdown","source":["### Inspect ratio of missing value for each column"],"metadata":{"colab_type":"text","id":"14RUHQafaiGt"}},{"cell_type":"code","source":["missing_ratio_3 = calc_missing_ratio(result)\nmissing_ratio_3 = missing_ratio_3.loc[(missing_ratio_3 > 0.02).all(axis=1), :]\nplot_missing_freq(missing_ratio_3, 'Feature', 'Missing Value Ratio', 'Airbnb Data Set Missing Value')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":379},"colab_type":"code","id":"ZeM7AAyVYmfd","outputId":"57c8f3bc-c559-49af-c4af-4d008cd5cd4b"},"outputs":[],"execution_count":65},{"cell_type":"markdown","source":["### Drop sentiment comment related columns"],"metadata":{"colab_type":"text","id":"Uup5nxqCxedw"}},{"cell_type":"markdown","source":["- It is possible that review related values missing are due to lacking of guest, to aviod bias, the records that has no review related data will be dropped."],"metadata":{"colab_type":"text","id":"16iQcUntdhqT"}},{"cell_type":"code","source":["@singleton\nclass MissingReviewRecordRemover(Transformer):\n  def __init__(self):\n    pass\n\n  def _transform(self, df: DataFrame) -> DataFrame:\n    result = df\n\n    review_columns = [*[col_name for col_name in result.columns if col_name.startswith('review')], 'id']\n    drop_rate = (1 - result.select(review_columns).dropna().count() / result.count()) * 100\n    review_dropped_result = result.select(review_columns).dropna().select(fn.col('id').alias('_id'))\n    result = review_dropped_result.join(result, review_dropped_result._id == result.id).drop('_id')\n\n    return result\n\nmissing_review_record_remover = MissingReviewRecordRemover()\nwrangling_pipeline = Pipeline(stages=[bool_converter\n                            , currency_converter\n                            , rate_converter\n                            , zero_variance_cleaner\n                            , literature_column_remover\n                            , url_column_remover\n                            , incomplete_column_remover\n                            , sentement_analysis_column_remover\n                            , missing_review_record_remover])\nresult = wrangling_pipeline.fit(df).transform(df)\n\nshape(result)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":34},"colab_type":"code","id":"tpUS4F-Bd89M","outputId":"75b0383d-0afa-4037-be12-3105940094f1"},"outputs":[],"execution_count":68},{"cell_type":"code","source":["missing_ratio_4 = calc_missing_ratio(result)\nplot_missing_freq(missing_ratio_4, 'Feature', 'Missing Value Ratio', 'Airbnb Data Set Missing Value')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":396},"colab_type":"code","id":"axldrk35bYGX","outputId":"17a4ff36-312b-4394-daac-1330f14a59fe"},"outputs":[],"execution_count":69},{"cell_type":"markdown","source":["## Missing value imputation"],"metadata":{"colab_type":"text","id":"ym3LTQNHxtBk"}},{"cell_type":"markdown","source":["- Impute the rest missing values with mean for numerical columns and mode for boolean columns"],"metadata":{"colab_type":"text","id":"o6sP0FaXkdNT"}},{"cell_type":"code","source":["@singleton\nclass MissingValueImputer(Transformer):\n  def __init__(self):\n    self._reg_exp = re.compile(\"avg\\((.*)\\)\", re.IGNORECASE)\n    self._reg_exp_2 = re.compile(\"mode\\((.*)\\)\", re.IGNORECASE)\n    self._excuded_columns = ['id', 'host_id', 'host_is_superhost', 'longitude', 'latitude', 'is_location_exact',\n                             'instant_bookable', 'require_guest_profile_picture', 'require_guest_phone_verification']\n    self._boolean_columns = ['host_is_superhost', 'is_location_exact', 'instant_bookable', \n                       'require_guest_profile_picture', 'require_guest_phone_verification']\n\n  def _transform(self, df: DataFrame) -> DataFrame:\n    result = df\n\n    numerical_columns = [f.name for f in result.schema.fields if isinstance(f.dataType, t.NumericType)]\n    numerical_columns = [col_name for col_name in numerical_columns if col_name not in self._excuded_columns]\n\n    means = result.agg({col_name: 'mean' for col_name in numerical_columns})\n    for col_name in means.columns:\n      means = means.withColumnRenamed(col_name, f'_{self._reg_exp.search(col_name).group(1)}')\n\n    dummy_link = fn.udf(lambda x:1, t.IntegerType())\n    result = result.withColumn('dummy_link', dummy_link(result.columns[0]))\n    means = means.withColumn('dummy_link', dummy_link(means.columns[0]))\n    result = result.join(means, on='dummy_link', how='inner')\n\n    for col_name in numerical_columns:\n      result = result.withColumn(col_name, fn.coalesce(fn.col(col_name), fn.col(f'_{col_name}')))\n    mode_map = dict()\n    for col_name in self._boolean_columns:\n      if col_name not in result.columns:\n        continue\n      cnts = result.groupBy(col_name).count()\n      mode = cnts.join(\n          cnts.agg(fn.max(\"count\").alias(\"max_\")), fn.col(\"count\") == fn.col(\"max_\")\n      ).limit(1).select(col_name)\n      mode = mode.first()[0]\n      mode_map[col_name]=mode\n\n    result = result.drop(*means.columns)\n    result = result.fillna(mode_map)\n\n    return result\n\n\nmissing_value_imputer = MissingValueImputer()\nwrangling_pipeline = Pipeline(stages=[bool_converter\n                            , currency_converter\n                            , rate_converter\n                            , zero_variance_cleaner\n                            , literature_column_remover\n                            , url_column_remover\n                            , incomplete_column_remover\n                            , sentement_analysis_column_remover\n                            , missing_review_record_remover\n                            , missing_value_imputer])\nresult = wrangling_pipeline.fit(df).transform(df)\n\nprint(f'Number of remaining value: {result.count() - result.dropna().count()}')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":34},"colab_type":"code","id":"ZwuYS9DqQIcA","outputId":"707d86f4-2fd2-4afb-a823-644c1fff3ef3"},"outputs":[],"execution_count":72},{"cell_type":"markdown","source":["- Remove the 4 missing value"],"metadata":{"colab_type":"text","id":"PRixyNQ6lpwM"}},{"cell_type":"code","source":["@singleton\nclass MissingValueImputer(Transformer):\n  def __init__(self):\n    self._reg_exp = re.compile(\"avg\\((.*)\\)\", re.IGNORECASE)\n\n  def _transform(self, df: DataFrame) -> DataFrame:\n    result = df\n\n    numerical_columns = [f.name for f in result.schema.fields if isinstance(f.dataType, t.NumericType)]\n\n    means = result.agg({col_name: 'mean' for col_name in numerical_columns})\n    for col_name in means.columns:\n      means = means.withColumnRenamed(col_name, f'_{self._reg_exp.search(col_name).group(1)}')\n\n    dummy_link = fn.udf(lambda x:1, t.IntegerType())\n    result = result.withColumn('dummy_link', dummy_link(result.columns[0]))\n    means = means.withColumn('dummy_link', dummy_link(means.columns[0]))\n    result = result.join(means, on='dummy_link', how='inner')\n\n    for col_name in numerical_columns:\n      result = result.withColumn(col_name, fn.coalesce(fn.col(col_name), fn.col(f'_{col_name}')))\n    result = result.drop(*means.columns).dropna()\n\n    return result\n\nmissing_value_imputer = MissingValueImputer()\nwrangling_pipeline = Pipeline(stages=[bool_converter\n                            , currency_converter\n                            , rate_converter\n                            , zero_variance_cleaner\n                            , literature_column_remover\n                            , url_column_remover\n                            , incomplete_column_remover\n                            , sentement_analysis_column_remover\n                            , missing_review_record_remover\n                            , missing_value_imputer])\nresult = wrangling_pipeline.fit(df).transform(df)"],"metadata":{"colab":{},"colab_type":"code","id":"CGLUIWH9luKi"},"outputs":[],"execution_count":74},{"cell_type":"code","source":["display(result)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":224},"colab_type":"code","id":"DQxkl0Lxl9Lx","outputId":"82b6cc0e-ec0a-4589-b3f1-92c59c1d2b70"},"outputs":[],"execution_count":75},{"cell_type":"code","source":["id_date = ['id','host_since','host_id','first_review','last_review','_c0']\nid_date_column_remover = ColumnRemover(id_date)\nwrangling_pipeline = Pipeline(stages=[bool_converter\n                            , currency_converter\n                            , rate_converter\n                            , zero_variance_cleaner\n                            , literature_column_remover\n                            , url_column_remover\n                            , incomplete_column_remover\n                            , sentement_analysis_column_remover\n                            , missing_review_record_remover\n                            , missing_value_imputer\n                            ,id_date_column_remover])\nresult = wrangling_pipeline.fit(df).transform(df)"],"metadata":{"colab":{},"colab_type":"code","id":"1Wnq52i_xIGz"},"outputs":[],"execution_count":76},{"cell_type":"code","source":["#Save the dataframe to csv document for regression model\n#df_pd = result.toPandas()\n#df_pd.to_csv(\"/content/drive/Shared drives/718_project/dataset/listing_clean_new(used_for_random_forest_and_GBT).csv\")"],"metadata":{"colab":{},"colab_type":"code","id":"wMI-ibdMxsXT"},"outputs":[],"execution_count":77},{"cell_type":"markdown","source":["## Outlier Imputation"],"metadata":{"colab_type":"text","id":"JAp33APpnAgk"}},{"cell_type":"markdown","source":["### Distribution inspection"],"metadata":{"colab_type":"text","id":"ohF0U9faAGLd"}},{"cell_type":"markdown","source":["#### boxplot"],"metadata":{"colab_type":"text","id":"IcQ_Qvv_APPj"}},{"cell_type":"markdown","source":["- In the big data environment, extracting data from the distributed file system is infeasible. To se the distribution of each feature, we will calculate the descriptive information of the dataset"],"metadata":{"colab_type":"text","id":"8B8dTo1Q57gb"}},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n\ndef plot_feature_distribution(result):\n  non_continuous_columns = ['host_is_superhost', 'host_has_profile_pic', 'host_identity_verified',\n                            'is_location_exact', 'guests_included', 'instant_bookable',\n                            'latitude', 'longitude']\n\n  numerical_columns = [f.name for f in result.schema.fields if isinstance(f.dataType, t.NumericType)]\n  numerical_columns = [col_name for col_name in numerical_columns if col_name not in non_continuous_columns]\n\n  stats = list()\n  for col_name in numerical_columns:\n    percentiles = result.select(col_name).agg(\n            fn.expr(f'percentile({col_name}, array(0))').alias('0%'),\n            fn.expr(f'percentile({col_name}, array(0.25))').alias('25%'), \n            fn.expr(f'percentile({col_name}, array(0.5))').alias('50%'), \n            fn.expr(f'percentile({col_name}, array(0.75))').alias('75%'),\n            fn.expr(f'percentile({col_name}, array(1))').alias('100%')).toPandas()\n            \n    _0 = percentiles.loc[0, '0%']\n    _25 = percentiles.loc[0, '25%']\n    _50 = percentiles.loc[0, '50%']\n    _75 = percentiles.loc[0, '75%']\n    _100 = percentiles.loc[0, '100%']\n    stats.append({'label': col_name, 'med': _50, 'q1': _25, 'q3': _75, 'whislo': _0, 'whishi': _100})\n\n  fig, ax = plt.subplots(len(numerical_columns), 1, sharex=False, sharey='row', figsize=(15, 50))\n\n  for idx, s in enumerate(stats):\n    ax[idx].bxp([stats[idx]], vert=False, showfliers=False);\n\n  plt.tight_layout()\n\n  display()\n\n\nplot_feature_distribution(result)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"colab_type":"code","id":"G0ODQLXt6Q8F","outputId":"0980bcc4-3872-4a84-8fb6-bfee8fc8a874"},"outputs":[],"execution_count":82},{"cell_type":"markdown","source":["- Some features are not normal distributed in the data space. Convert them by taking logarithm."],"metadata":{"colab_type":"text","id":"t3KTV7i6l8u9"}},{"cell_type":"code","source":["import math\n\n\nclass LogarithmImputer(Transformer):\n  def __init__(self):\n    self._excluded_columns = ['id', 'host_id', 'host_is_superhost', 'host_has_profile_pic',\n                              'host_identity_verified', 'is_location_exact',\n                              'guests_included', 'instant_bookable', \n                              'latitude', 'longitude', 'availability_30', \n                              'availability_60', 'availability_90', 'availability_365', \n                              ]\n    self._transofrmed_columns = list()\n    self._log = fn.udf(lambda x: math.log(x+1), t.DoubleType())\n\n  @property\n  def transofrmed_columns(self):\n    return self._transofrmed_columns\n\n  def _transform(self, df: DataFrame) -> DataFrame:\n    result = df\n\n    numerical_columns = [f.name for f in result.schema.fields if isinstance(f.dataType, t.NumericType)]\n    numerical_columns = [col_name for col_name in numerical_columns if col_name not in self._excluded_columns]\n    self._transofrmed_columns = numerical_columns\n\n    for col_name in self._transofrmed_columns:\n\n      result = result.withColumn(col_name, self._log(col_name))\n\n    return result\n\nlogarithm_imputer = LogarithmImputer()\nlogarithm_pipeline = Pipeline(stages=[bool_converter\n                            , currency_converter\n                            , rate_converter\n                            , zero_variance_cleaner\n                            , literature_column_remover\n                            , url_column_remover\n                            , incomplete_column_remover\n                            , sentement_analysis_column_remover\n                            , missing_review_record_remover\n                            , missing_value_imputer\n                            , logarithm_imputer])\n\nresult = logarithm_pipeline.fit(df).transform(df)\n\nplot_feature_distribution(result.select(logarithm_imputer.transofrmed_columns))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"colab_type":"code","id":"l1VPTSwV6QXT","outputId":"8fa4aa52-72ee-4740-cd77-04e2f1b5e4f8"},"outputs":[],"execution_count":84},{"cell_type":"markdown","source":["### Winsorizing the data set"],"metadata":{"colab_type":"text","id":"ARoZyCNWnilK"}},{"cell_type":"markdown","source":["- host id is a categorical feature, it should be revised"],"metadata":{"colab_type":"text","id":"djLz-ALMAcfk"}},{"cell_type":"code","source":["@singleton\nclass WinsorizingImputer(Transformer):\n  def __init__(self):\n    self._pending_columns = [f.name for f in result.schema.fields if isinstance(f.dataType, t.NumericType)]\n    self._excuded_columns = ['id', 'host_id', 'host_is_superhost', 'longitude', 'latitude', 'is_location_exact',\n                             'instant_bookable', 'require_guest_profile_picture', 'require_guest_phone_verification',\n                             'price' ]\n    def winsorize(tail, head):\n      def _winsorize(value):\n        if value < tail:\n          return float(tail)\n        if value > head:\n          return float(head)\n        return float(value)\n      return _winsorize\n    self._winsorize = lambda col, tail, head: fn.udf(winsorize(tail, head), t.DoubleType())(col)\n\n  @property\n  def pending_columns(self):\n    return self._pending_columns\n\n  def _transform(self, df: DataFrame) -> DataFrame:\n    result = df\n\n    self._pending_columns = [f.name for f in result.schema.fields if isinstance(f.dataType, t.NumericType)]\n    self._pending_columns = [col_name for col_name in self._pending_columns if col_name not in self._excuded_columns]\n\n    for col_name in self._pending_columns:\n      percentiles = result.agg(\n          fn.expr(f'percentile({col_name}, array(0.25))').alias('tail'), \n          fn.expr(f'percentile({col_name}, array(0.75))').alias('head')).toPandas()\n      tail = percentiles.loc[0, 'tail'][0]\n      head = percentiles.loc[0, 'head'][0]\n      result = result.withColumn(col_name, self._winsorize(col_name, tail, head))\n    result = result.drop(*percentiles.columns)\n\n    return result\n\nwinsorizing_imputer = WinsorizingImputer()\nwrangling_pipeline = Pipeline(stages=[bool_converter\n                            , currency_converter\n                            , rate_converter\n                            , zero_variance_cleaner\n                            , literature_column_remover\n                            , url_column_remover\n                            , incomplete_column_remover\n                            , sentement_analysis_column_remover\n                            , missing_review_record_remover\n                            , missing_value_imputer\n                            , logarithm_imputer\n                            , winsorizing_imputer])\nresult = wrangling_pipeline.fit(df).transform(df)"],"metadata":{"colab":{},"colab_type":"code","id":"s8xZQyP1nDDZ"},"outputs":[],"execution_count":87},{"cell_type":"markdown","source":["### Low Variance Features Removal"],"metadata":{"colab_type":"text","id":"PUETRctEdrGY"}},{"cell_type":"code","source":["from pyspark.ml import feature\n\n\n@singleton\nclass LowVarianceCleaner(Transformer):\n  def __init__(self):\n    self._reg_exp = re.compile(\"variance\\((.*)\\)\", re.IGNORECASE)\n    self._pending_columns = list()\n    self._excuded_columns = ['id', 'host_id', 'host_is_superhost', 'longitude', 'latitude', 'is_location_exact',\n                             'instant_bookable', 'require_guest_profile_picture', 'require_guest_phone_verification']\n    self._extract_udf = lambda idx: fn.udf(lambda centered_features: float(centered_features[idx]), t.DoubleType())('centered_features')\n    self._threshold = 0.01\n    self._low_variance_columns = list()\n\n  @property\n  def pending_columns(self):\n    return self._pending_columns\n\n  @property\n  def low_variance_columns(self):\n    return self._low_variance_columns\n\n  def _transform(self, df: DataFrame) -> DataFrame:\n    result = df\n\n    self._pending_columns = [f.name for f in result.schema.fields if isinstance(f.dataType, t.NumericType)]\n    self._pending_columns = [col_name for col_name in self._pending_columns if col_name not in self._excuded_columns]\n\n    standardized_result = Pipeline(stages=[feature.VectorAssembler(inputCols=self._pending_columns, outputCol='features'),\n                                            feature.StandardScaler(withMean=True, withStd=True, inputCol='features', outputCol='centered_features')]) \\\n          .fit(result).transform(result)\n\n    for idx, col_name in enumerate(self._pending_columns):\n      standardized_result = standardized_result.withColumn(col_name, self._extract_udf(idx))\n    standardized_result = standardized_result.drop('centered_features')\n\n    numeric_variances = standardized_result.agg({col_name : 'variance' for col_name in self._pending_columns})\n    numeric_variances = numeric_variances.select(*[fn.col(col).alias(self._reg_exp.search(col).group(1)) for col in numeric_variances.columns]) \\\n      .toPandas().T.iloc[:,0]\n    numeric_variances = numeric_variances[(numeric_variances < self._threshold) | (numeric_variances.isna())]\n\n    self._low_variance_columns = numeric_variances.index\n\n    result = result.drop(*self._low_variance_columns)\n\n    return result\n\n\nlow_variance_cleaner = LowVarianceCleaner()\nwrangling_pipeline = Pipeline(stages=[bool_converter\n                            , currency_converter\n                            , rate_converter\n                            , zero_variance_cleaner\n                            , literature_column_remover\n                            , url_column_remover\n                            , incomplete_column_remover\n                            , sentement_analysis_column_remover\n                            , missing_review_record_remover\n                            , missing_value_imputer\n                            , logarithm_imputer\n                            , winsorizing_imputer\n                            , low_variance_cleaner])\n\nresult = wrangling_pipeline.fit(df).transform(df)\n\nprint(shape(result))\n\n#return"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":34},"colab_type":"code","id":"2z5Ww74-dt0m","outputId":"927a2443-f7ab-484b-83d7-b5e3d34f35b7"},"outputs":[],"execution_count":89},{"cell_type":"markdown","source":["## Feature Selection"],"metadata":{"colab_type":"text","id":"TuH3nOkIzPf8"}},{"cell_type":"markdown","source":["### Tree-based algorithm"],"metadata":{"colab_type":"text","id":"TR2ahfWmKlhJ"}},{"cell_type":"code","source":["wrangling_pipeline = Pipeline(stages=[bool_converter\n                            , currency_converter\n                            , rate_converter\n                            , zero_variance_cleaner\n                            , literature_column_remover\n                            , url_column_remover\n                            , incomplete_column_remover\n                            , sentement_analysis_column_remover\n                            , missing_review_record_remover\n                            , missing_value_imputer\n                            , low_variance_cleaner])\n\nresult = wrangling_pipeline.fit(df).transform(df)"],"metadata":{"colab":{},"colab_type":"code","id":"of4IBzWAKkkx"},"outputs":[],"execution_count":92},{"cell_type":"markdown","source":["#### Random Forest\n- Explore feature importance By using Random Forest Regressor"],"metadata":{"colab_type":"text","id":"g5o6pzyBI907"}},{"cell_type":"code","source":["from pyspark.sql import SparkSession\nimport pyspark.sql.functions as fn\nfrom matplotlib import pyplot as plt\nimport numpy as np\nimport pandas as pd\nspark = SparkSession.builder.getOrCreate()\nsc = spark.sparkContext"],"metadata":{"colab":{},"colab_type":"code","id":"1JjFOpX_G_QF"},"outputs":[],"execution_count":94},{"cell_type":"code","source":["result = spark.read\\\n        .option(\"header\", \"true\")\\\n        .option(\"multiLine\", \"true\")\\\n        .option('inferSchema', 'true')\\\n        .option('escape', '\"')\\\n        .csv(get_training_filename('listing_clean_new(used_for_random_forest_and_GBT).csv'))"],"metadata":{"colab":{},"colab_type":"code","id":"1xDyGLYpHB2R"},"outputs":[],"execution_count":95},{"cell_type":"code","source":["from pyspark.ml.feature import HashingTF,CountVectorizer,IDF,StringIndexer,OneHotEncoder\ncategorical_cols_new = ['neighbourhood_group_cleansed','host_response_time',\n       'property_type', 'room_type', 'bed_type','cancellation_policy']\ncol_idx_new=['neighbourhood_group_cleansed_IDX',\n 'host_response_time_IDX',\n 'property_type_IDX',   \n 'room_type_IDX',\n 'bed_type_IDX',\n 'cancellation_policy_IDX']\n\nindexers_new = [StringIndexer(inputCol=col, outputCol = col + \"_IDX\")\\\n            .setHandleInvalid(\"keep\") for col in categorical_cols_new]"],"metadata":{"colab":{},"colab_type":"code","id":"ojhf_DMkfC14"},"outputs":[],"execution_count":96},{"cell_type":"code","source":["from pyspark.ml.feature import VectorAssembler\nfrom pyspark.ml.regression import RandomForestRegressor\ncol_numeric = [c for c in result.columns if ( c not in categorical_cols_new\n                                             and c not in ['host_verifications','amenities','price'])]\nassemble1 = VectorAssembler(inputCols= col_numeric + col_idx_new ,outputCol='features')\nreg = RandomForestRegressor(labelCol='price',featuresCol='features')"],"metadata":{"colab":{},"colab_type":"code","id":"xZ37loarfUHQ"},"outputs":[],"execution_count":97},{"cell_type":"code","source":["from pyspark.ml import Pipeline\ntransformer1 = Pipeline(stages=indexers_new  + [assemble1,reg])"],"metadata":{"colab":{},"colab_type":"code","id":"V2Qz7eE8hag6"},"outputs":[],"execution_count":98},{"cell_type":"code","source":["from pyspark.ml.tuning import CrossValidator\nfrom pyspark.ml.evaluation import RegressionEvaluator\nfrom pyspark.ml.tuning import ParamGridBuilder\n\ndf_train,df_test = result.randomSplit([0.7,0.3],seed=42)\n\nparamGrid = ParamGridBuilder() \\\n    .addGrid(reg.numTrees, [int(x) for x in np.linspace(start = 3, stop = 15, num = 3)]) \\\n    .addGrid(reg.maxDepth, [int(x) for x in np.linspace(start = 3, stop = 15, num = 3)]) \\\n    .build()\n\n\nevaluator = RegressionEvaluator(labelCol='price') \ncrossval = CrossValidator(estimator=transformer1,\n                          estimatorParamMaps=paramGrid,\n                          evaluator=evaluator,\n                          numFolds=3)"],"metadata":{"colab":{},"colab_type":"code","id":"4Fzdc4u1hxju"},"outputs":[],"execution_count":99},{"cell_type":"code","source":["cvModel = crossval.fit(df_train)"],"metadata":{"colab":{},"colab_type":"code","id":"H0841SJFibPL"},"outputs":[],"execution_count":100},{"cell_type":"code","source":["bestPipeline = cvModel.bestModel\nbestModel = bestPipeline.stages[-1]\nimportances = bestModel.featureImportances"],"metadata":{"colab":{},"colab_type":"code","id":"YXi_X_7ejo_Y"},"outputs":[],"execution_count":101},{"cell_type":"code","source":["fi_df = pd.DataFrame(importances.toArray(), columns=['importances'])\nfi_df['feature'] = pd.Series(col_numeric+col_idx_new)\nfi_df.sort_values(by=['importances'], ascending=False, inplace=True)"],"metadata":{"colab":{},"colab_type":"code","id":"-eQRvtwFjyAQ"},"outputs":[],"execution_count":102},{"cell_type":"code","source":["plt.figure(figsize=(20,10))\nfi_df.plot.barh(x='feature', \n               y ='importances',\n               figsize=(20,8), \n               title='Feature Importances', \n               fontsize=10)\ndisplay(plt.show())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":573},"colab_type":"code","id":"-46whMQMj_3v","outputId":"f4a68776-8b13-4372-9ae7-788853d1c446"},"outputs":[],"execution_count":103},{"cell_type":"markdown","source":["**Run random forest regression By filtering importance which is bigger than 0.03**"],"metadata":{"colab_type":"text","id":"JX315UO7j5ne"}},{"cell_type":"code","source":["fi_df_new = fi_df[fi_df['importances']>0.03]"],"metadata":{"colab":{},"colab_type":"code","id":"60_M-H5ilKEy"},"outputs":[],"execution_count":105},{"cell_type":"code","source":["fi_df_new"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":390},"colab_type":"code","id":"37ierHbBSV_x","outputId":"c5a2c2ec-c6eb-4b74-984e-cc48a28d597c"},"outputs":[],"execution_count":106},{"cell_type":"code","source":["num_feature_filtered = []\nfor i in fi_df_new['feature'].tolist():\n    if i not in ['neighbourhood_group_cleansed_IDX','cancellation_policy_IDX']:\n       num_feature_filtered.append(i)"],"metadata":{"colab":{},"colab_type":"code","id":"qV_UsvgjlfDQ"},"outputs":[],"execution_count":107},{"cell_type":"code","source":["num_feature_filtered"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":187},"colab_type":"code","id":"QVimHdfhT7YG","outputId":"12a8c7e6-7abf-47be-8c75-049096a5143b"},"outputs":[],"execution_count":108},{"cell_type":"code","source":["# change categorial col to Index\ncategorical_cols_filtered = ['neighbourhood_group_cleansed'\n                            ,'cancellation_policy']\nindexers_filtered = [StringIndexer(inputCol=col, outputCol = col + \"_IDX\")\\\n            .setHandleInvalid(\"keep\") for col in categorical_cols_filtered]\n\nencoded_filtered = [OneHotEncoder(inputCol = col + \"_IDX\", outputCol = col + '_Vec') for col in categorical_cols_filtered]"],"metadata":{"colab":{},"colab_type":"code","id":"tRHb3DwYhCCI"},"outputs":[],"execution_count":109},{"cell_type":"code","source":["from pyspark.ml.feature import CountVectorizer\nfrom pyspark.ml.feature import IDF\nfrom pyspark.ml.regression import RandomForestRegressor\n\ncv1 = CountVectorizer()\\\n    .setInputCol('host_verifications')\\\n    .setOutputCol('tf1')\n\ncv2 = CountVectorizer()\\\n    .setInputCol('amenities')\\\n    .setOutputCol('tf2')\n\nidf1 = IDF().\\\n    setInputCol(\"tf1\").\\\n    setOutputCol(\"tfidf1\")\n\nidf2 = IDF().\\\n    setInputCol(\"tf2\").\\\n    setOutputCol(\"tfidf2\")\n\nassemble1 = VectorAssembler(inputCols= num_feature_filtered, outputCol='features')\n\nassemble2 = VectorAssembler(inputCols= ['features','tfidf1','tfidf2','neighbourhood_group_cleansed_Vec'\n                                        ,'cancellation_policy_Vec'], outputCol='final_features')\n\nreg = RandomForestRegressor(labelCol='price',featuresCol='final_features')\n\ntransformer_final = Pipeline(stages=indexers_filtered + encoded_filtered + [cv1,idf1,cv2,idf2\n                                                                            ,assemble1,assemble2,reg])"],"metadata":{"colab":{},"colab_type":"code","id":"SiF8gDJwyBOo"},"outputs":[],"execution_count":110},{"cell_type":"code","source":["#just run one time\nfrom pyspark.sql.functions import array\nresult =result.withColumn('host_verifications', array(result['host_verifications']))\nresult =result.withColumn('amenities', array(result['amenities']))"],"metadata":{"colab":{},"colab_type":"code","id":"LS-j2n7rosdS"},"outputs":[],"execution_count":111},{"cell_type":"code","source":["df_train,df_test = result.randomSplit([0.7,0.3],seed=42)\nparamGrid_1 = ParamGridBuilder() \\\n    .addGrid(reg.numTrees, [int(x) for x in np.linspace(start = 5, stop = 15, num = 3)]) \\\n    .addGrid(reg.maxDepth, [int(x) for x in np.linspace(start = 5, stop = 15, num = 3)]) \\\n    .build()\n\nevaluator_1 = RegressionEvaluator(labelCol='price') \ncrossval_1 = CrossValidator(estimator=transformer_final,\n                          estimatorParamMaps=paramGrid_1,\n                          evaluator=evaluator_1,\n                          numFolds=3)\ncvModel_1 = crossval_1.fit(df_train)\nbestModel_1 = cvModel_1.bestModel\npreds_1 = bestModel_1.transform(df_test)"],"metadata":{"colab":{},"colab_type":"code","id":"i1_BPI3smrRK"},"outputs":[],"execution_count":112},{"cell_type":"code","source":["rmse = evaluator.evaluate(preds_1, {evaluator.metricName: 'rmse'})\nr2 = evaluator.evaluate(preds_1, {evaluator.metricName: 'r2'})\nprint(' RMSE: ' + str(rmse))\nprint(' R^2: ' + str(r2))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":51},"colab_type":"code","id":"BWVjKcfepPPv","outputId":"deadb560-8c43-49f0-ef03-75d508e9350e"},"outputs":[],"execution_count":113},{"cell_type":"code","source":["rfResult = preds_1.toPandas()\n\nplt.plot(rfResult.price, rfResult.prediction, 'bo')\nplt.xlabel('Price')\nplt.ylabel('Prediction')\nplt.suptitle(\"Model Performance RMSE: %f\" % rmse)\ndisplay(plt.show())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":314},"colab_type":"code","id":"jxf5aVTC-x-s","outputId":"f232cefd-6136-4aec-9281-412d6fc6e842"},"outputs":[],"execution_count":114},{"cell_type":"code","source":["print('numTrees - ', bestModel_1.stages[-1].getNumTrees)\nprint('maxDepth - ', bestModel_1.stages[-1].getOrDefault('maxDepth'))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":51},"colab_type":"code","id":"RbGFjxpsKRFo","outputId":"5d07cb70-44cc-4c5b-c035-308d6d2603dd"},"outputs":[],"execution_count":115},{"cell_type":"markdown","source":["#### Gradient Boosting Machine"],"metadata":{"colab_type":"text","id":"jWiEf5Esl0ef"}},{"cell_type":"code","source":["from pyspark.ml.regression import GBTRegressor\n\n\ngbt = GBTRegressor(labelCol='price',featuresCol='final_features')\n\ntransformer_final_gbt = Pipeline(stages=indexers_filtered + encoded_filtered + [cv1,idf1,cv2,idf2\n                                                                            ,assemble1,assemble2,gbt])"],"metadata":{"colab":{},"colab_type":"code","id":"5Q0F6fxwl6MH"},"outputs":[],"execution_count":117},{"cell_type":"code","source":["# We trained 70% of total data and test 30% of total data, but it cost 40 minutes by using GBT regression.\n# If it run too much time, databrick would stop, so I sample our data. Then everything can go smoothly in databricks\nresult_sample = result.sample(False,0.2,42)\ndf_train,df_test = result_sample.randomSplit([0.7,0.3],seed=42)\nparamGrid_2 = ParamGridBuilder() \\\n    .addGrid(gbt.maxIter, [int(x) for x in np.linspace(start = 5, stop = 15, num = 3)]) \\\n    .addGrid(gbt.maxDepth, [int(x) for x in np.linspace(start = 5, stop = 15, num = 3)]) \\\n    .build()\n\nevaluator_2 = RegressionEvaluator(labelCol='price') \ncrossval_2 = CrossValidator(estimator=transformer_final_gbt,\n                          estimatorParamMaps=paramGrid_2,\n                          evaluator=evaluator_2,\n                          numFolds=3)\ncvModel_2 = crossval_2.fit(df_train)\nbestModel_2 = cvModel_2.bestModel\npreds_2 = bestModel_2.transform(df_test)"],"metadata":{},"outputs":[],"execution_count":118},{"cell_type":"code","source":["rmse = evaluator.evaluate(preds_2, {evaluator.metricName: 'rmse'})\nr2 = evaluator.evaluate(preds_2, {evaluator.metricName: 'r2'})\nprint(' RMSE: ' + str(rmse))\nprint(' R^2: ' + str(r2))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":51},"colab_type":"code","id":"JIhFYspxmB49","outputId":"d3b3bc2f-2b2e-4836-b9b3-61d6369fe86a"},"outputs":[],"execution_count":119},{"cell_type":"code","source":["rfResult = preds_2.toPandas()\n\nplt.plot(rfResult.price, rfResult.prediction, 'bo')\nplt.xlabel('Price')\nplt.ylabel('Prediction')\nplt.suptitle(\"GBT Model Performance RMSE: %f\" % rmse)\ndisplay(plt.show())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":314},"colab_type":"code","id":"mhTotZ3DmDts","outputId":"e8a018ad-8161-4315-cc5c-185592bb410f"},"outputs":[],"execution_count":120},{"cell_type":"code","source":["print('numIter - ', bestModel_2.stages[-1].getOrDefault('maxIter'))\nprint('maxDepth - ', bestModel_2.stages[-1].getOrDefault('maxDepth'))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"colab_type":"code","id":"w1S8bIZPKX3t","outputId":"9a036b29-f396-493b-ac33-c2d164d661e7"},"outputs":[],"execution_count":121},{"cell_type":"markdown","source":["### Lasso regression"],"metadata":{"colab_type":"text","id":"YSFsTj9tzNyx"}},{"cell_type":"markdown","source":["- Features reduce quickly in the lasso model. For a dataset with high dimensional space, it is hard to find a close relationshi among observations. Under the L1 penalty term regularization, the lasso model will filter the most relevant features. \n- In the elastic net regression, the model becomes lasso regression when α equals to 1, which takes the L1 penalty term only. \n- The scree plot direct us the reasonable λ to apply on the penaly term."],"metadata":{"colab_type":"text","id":"KoAw5YD2zprP"}},{"cell_type":"code","source":["from pyspark.ml import regression, tuning, evaluation\n\n\ndef build_index_transformer(columns):\n  return [StringIndexer(inputCol=col, outputCol = col + \"_IDX\", handleInvalid=\"keep\")  for col in columns]\n\ncategorical_type_features = ['property_type', 'room_type', 'bed_type']\nindexers = build_index_transformer([col_name for col_name in categorical_type_features if col_name in df.columns])\ncategorical_type_features_remover = ColumnRemover(categorical_type_features)\nnon_numerical_column_remover = ColumnRemover(['id', 'host_id', 'host_since', 'first_review', 'last_review', 'host_response_time',\n                                              'host_verifications', 'neighbourhood_group_cleansed',  'amenities', 'cancellation_policy', \n                                              'latitude', 'longitude'])\n\nlasso_dataset_generate_pipeline = Pipeline(stages=[bool_converter\n                            , currency_converter\n                            , rate_converter\n                            , zero_variance_cleaner\n                            , literature_column_remover\n                            , url_column_remover\n                            , incomplete_column_remover\n                            , sentement_analysis_column_remover\n                            , missing_review_record_remover\n                            , non_numerical_column_remover\n                            , missing_value_imputer\n                            , logarithm_imputer\n                            , winsorizing_imputer\n                            , low_variance_cleaner\n                            , *indexers\n                            , categorical_type_features_remover])\n\n\nresult = lasso_dataset_generate_pipeline.fit(df).transform(df)\n\nprint(f'Lasso dataset shape: {shape(result)}')\n\ntrain_data, test_data = split_data(result)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":34},"colab_type":"code","id":"D7YZvcPC0ia0","outputId":"a53cf90e-e7ea-4417-ad64-6ea1472ec988"},"outputs":[],"execution_count":124},{"cell_type":"markdown","source":["- Training Lasso Model and visualize the performance via scree plot"],"metadata":{"colab_type":"text","id":"4_HakzpgSFzu"}},{"cell_type":"code","source":["# Clean the data\n_lambda = pd.Series(np.arange(0.1,0.34,0.02)).tolist()\n\n_train_r2 = list()\n_test_r2 = list()\n_coef = list()\ndef search_lambda(train_data, lbds, train_r2, test_r2, coef):\n  assembler = feature.VectorAssembler(inputCols=train_data.drop('price').columns, outputCol='features')\n  standardizer = feature.StandardScaler(withMean=True, withStd=True, inputCol=\"features\", outputCol=\"scaled_features\")\n  normalizer = feature.Normalizer(inputCol=\"scaled_features\", outputCol=\"norm_features\", p=2.0)\n\n  lr = regression.LinearRegression() \\\n    .setLabelCol('price') \\\n    .setFeaturesCol('norm_features') \\\n    .setMaxIter(10) \\\n    .setElasticNetParam(1)\n\n  lasso_cleaning_pipeline = Pipeline(stages=[assembler, standardizer, normalizer, lr])\n\n  evaluator = evaluation.RegressionEvaluator() \\\n    .setLabelCol(lasso_cleaning_pipeline.getStages()[-1].getLabelCol()) \\\n    .setMetricName('r2')\n\n\n\n  for lbd in lbds:\n    paramGrid = tuning.ParamGridBuilder()\\\n        .addGrid(lr.regParam, [lbd]) \\\n        .build()\n\n    tvs = tuning.TrainValidationSplit(estimator=lasso_cleaning_pipeline,\n                              estimatorParamMaps=paramGrid,\n                              evaluator=evaluator,\n                              trainRatio=0.7)\n\n\n    model = tvs.fit(train_data)\n\n    train_r2.append(evaluator.evaluate(model.transform(train_data)))\n    test_r2.append(evaluator.evaluate(model.transform(test_data)))\n    coef.append(model.bestModel.stages[-1].coefficients.toArray())\n\nsearch_lambda(train_data, _lambda, _train_r2, _test_r2, _coef)"],"metadata":{"colab":{},"colab_type":"code","id":"0I6ohtJRSCQB"},"outputs":[],"execution_count":126},{"cell_type":"markdown","source":["- Visualize performances of each lambda"],"metadata":{"colab_type":"text","id":"rm2TWyJznbUU"}},{"cell_type":"code","source":["def polt_model_performance(lbds, train_coef, test_coef, title_xys_1, title_xys_2):\n  plt.clf()\n  fig, ax = plt.subplots(2, 1, figsize=(20, 7))\n  ax[0].plot(lbds, train_coef, 'o-', linewidth=2, label=\"Training Set\")\n  ax[0].plot(lbds, test_coef, 'o-', linewidth=2, label=\"Test Set\")\n  ax[0].set_title(title_xys_1[0], fontSize=25)\n  ax[0].set_xlabel(title_xys_1[1])\n  ax[0].set_ylabel(title_xys_1[2])\n  ax[1].plot(lbds, [pair[0] - pair[1] for pair in zip(train_coef, test_coef)], 'o-', linewidth=2, label=f\"{title_xys_2[2]}\")\n  ax[1].set_title(title_xys_2[0], fontSize=25)\n  ax[1].set_xlabel(title_xys_2[1])\n  ax[1].set_ylabel(title_xys_2[2])\n  ax[0].legend()\n  ax[1].legend()\n\n  plt.tight_layout()\n  display()\n\npolt_model_performance(_lambda, _train_r2, _test_r2, \n                       ('Model Performance of Lasso Regression', 'Lambda', 'R^2'),\n                       ('Difference of R^2 Between Training Set and Test Set', 'Lambda', 'Difference of R^2'))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":481},"colab_type":"code","id":"y9CbCwpGnrXE","outputId":"a04aedaa-e5d6-454e-b6f2-0a52e3d9d11d"},"outputs":[],"execution_count":128},{"cell_type":"markdown","source":["- Trending of the weight for each features"],"metadata":{"colab_type":"text","id":"2CCxW_wcAOSw"}},{"cell_type":"code","source":["\n__coef = np.array(_coef)\n\ndef plot_importance(column_names, lbds, coef_matrix, threshold, title, x_lab, y_lab):\n  plt.clf()\n  plt.figure(figsize=(20, 17))\n  for idx, col_name in enumerate(column_names):\n    plt.plot(lbds, coef_matrix[:,idx], 'o-', linewidth=2, label=col_name)\n    c = coef_matrix[0,idx]\n    if abs(c) > threshold:\n      plt.annotate(col_name, (lbds[4], coef_matrix[4,idx]))\n\n  plt.title(title, fontSize=25)\n  plt.xlabel(x_lab)\n  plt.ylabel(y_lab)\n\n  plt.legend(loc='upper right')\n  plt.tight_layout()\n  display()\n\nplot_importance(train_data.drop('price').columns, _lambda, __coef, 0.25, 'Weight change on each feature', 'Lambda', 'Weight')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"colab_type":"code","id":"aB_Dg6xcANhM","outputId":"2eea9b1d-9e89-47aa-c767-0a6bbb1c040f"},"outputs":[],"execution_count":130},{"cell_type":"markdown","source":["- According to the scree plot, nearly all weights of features have been penalized to zero. To get a better insight of the trend of features, we will narrow down the lambda search range and run again."],"metadata":{"colab_type":"text","id":"YSQTX7J5HOrO"}},{"cell_type":"code","source":["_lambda = pd.Series(np.arange(-10.5,-3,1)).apply(math.exp).sort_values().tolist()\n_train_r2 = list()\n_test_r2 = list()\n_coef = list()\n\nsearch_lambda(train_data, _lambda, _train_r2, _test_r2, _coef)"],"metadata":{"colab":{},"colab_type":"code","id":"H4D9I0LXHS0c"},"outputs":[],"execution_count":132},{"cell_type":"code","source":["__coef = np.array(_coef)\n\nplot_importance(train_data.drop('price').columns, _lambda, __coef, 0.4, 'Weight change on each feature', 'Lambda', 'Weight')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"colab_type":"code","id":"TIrRJCAnInfB","outputId":"c5c1fcb6-4812-499b-f84e-9a250e88c397"},"outputs":[],"execution_count":133},{"cell_type":"markdown","source":["- Set 0.25 as the threshold. When lambda is near 0.01, the features with absolute weight above this number when will be the important features of our model."],"metadata":{"colab_type":"text","id":"betHGgdSHeuh"}},{"cell_type":"code","source":["idx = len(_lambda) - len([lbd for lbd in _lambda if lbd > 0.01])\n\nlass_weight = pd.DataFrame({'weight': __coef[idx,:], 'abs': np.abs(__coef[idx,:])}, \n                           index=train_data.drop('price').columns)\nlass_weight = lass_weight.sort_values(by='abs', ascending=False)\n\nfeasible_feature = lass_weight[lass_weight['abs'] > 0.25].index.values.tolist()\nfeasible_feature = [col_name.replace('_IDX', '') for col_name in feasible_feature]\nprint(*feasible_feature, sep='\\n')\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"colab_type":"code","id":"MOZzq5VUSQN-","outputId":"b1f4eeb2-a28c-4666-a730-ef99e72f1d7f"},"outputs":[],"execution_count":135},{"cell_type":"markdown","source":["The model fits the data space pretty well, so the regression coefficient is credible to filter out the useful features."],"metadata":{"colab_type":"text","id":"y4kay0utHoaC"}},{"cell_type":"markdown","source":["# Modeling"],"metadata":{"colab_type":"text","id":"HRGSKx1R4kqj"}},{"cell_type":"markdown","source":["## Data Partition"],"metadata":{"colab_type":"text","id":"-I9FoaQF4n69"}},{"cell_type":"markdown","source":["## Elastic Net Regression"],"metadata":{"colab_type":"text","id":"MVuTr0WAh5_a"}},{"cell_type":"markdown","source":["- Build the dataset with useful features only"],"metadata":{"colab_type":"text","id":"HPhuewVHXiFC"}},{"cell_type":"code","source":["@singleton\nclass ColumnSelector(Transformer):\n\n  def __init__(self, selected_columns):\n    self._selected_columns = selected_columns + ['id', 'price']\n\n  @property\n  def selected_columns(self):\n    return self._selected_columns\n\n  def _transform(self, df: DataFrame) -> DataFrame:\n    result = df\n    result = result.select(self._selected_columns)\n    return result\n\ndf_lasso = load_data('listings.csv')\n\ncolumn_selector = ColumnSelector(feasible_feature)\nindexers = build_index_transformer([col_name for col_name in categorical_type_features if col_name in feasible_feature])\n\n\nelasticnet_pipeline = Pipeline(stages=[column_selector\n                            , bool_converter\n                            , currency_converter\n                            , rate_converter\n                            , zero_variance_cleaner\n                            , literature_column_remover\n                            , url_column_remover\n                            , incomplete_column_remover\n                            , sentement_analysis_column_remover\n                            , missing_review_record_remover\n                            , non_numerical_column_remover\n                            , missing_value_imputer\n                            , logarithm_imputer\n                            , winsorizing_imputer\n                            , low_variance_cleaner\n                            , *indexers\n                            , categorical_type_features_remover\n                            ])\n\n\nresult = elasticnet_pipeline.fit(df_lasso).transform(df_lasso)\n\nprint(shape(result))\n\ntrain_data, test_data = split_data(result)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":34},"colab_type":"code","id":"Upzo540UXgh1","outputId":"04cd074c-2335-4d12-f423-3d5f6fb0e884"},"outputs":[],"execution_count":141},{"cell_type":"code","source":["result.columns"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"colab_type":"code","id":"zVH6JJFhuEJj","outputId":"92e7853f-1d35-4f0d-dea9-f7a853b4a70a"},"outputs":[],"execution_count":142},{"cell_type":"code","source":["_elastic_net_train_r2 = list()\n_elastic_net_test_r2 = list()\n_elastic_net_coef = list()\n\n_elastic_net_alpha = np.arange(0,1,.3)\n_elastic_net_lambda = np.arange(.05,.95,.15)\n\ndef search_lambda_alpha(train_data, test_data, lbds, alphas, train_r2, test_r2, coef):\n  assembler = feature.VectorAssembler(inputCols=train_data.drop('price').columns, outputCol='features')\n  standardizer = feature.StandardScaler(withMean=True, withStd=True, inputCol=\"features\", outputCol=\"scaled_features\")\n  normalizer = feature.Normalizer(inputCol=\"scaled_features\", outputCol=\"norm_features\", p=2.0)\n\n  lr = regression.LinearRegression() \\\n    .setLabelCol('price') \\\n    .setFeaturesCol('norm_features') \\\n    .setMaxIter(10) \\\n    .setElasticNetParam(1)\n\n  lasso_cleaning_pipeline = Pipeline(stages=[assembler, standardizer, normalizer, lr])\n\n  evaluator = evaluation.RegressionEvaluator() \\\n    .setLabelCol(lasso_cleaning_pipeline.getStages()[-1].getLabelCol()) \\\n    .setMetricName('r2')\n\n\n\n  # print(*train_data.columns, sep='\\n')\n  for a in alphas:\n    for lbd in lbds:\n      paramGrid = tuning.ParamGridBuilder()\\\n          .addGrid(lr.regParam, [lbd]) \\\n          .addGrid(lr.elasticNetParam, [a]) \\\n          .build()\n\n      tvs = tuning.TrainValidationSplit(estimator=lasso_cleaning_pipeline,\n                                estimatorParamMaps=paramGrid,\n                                evaluator=evaluator,\n                                trainRatio=0.7)\n\n      model = tvs.fit(train_data)\n\n      train_r2.append(evaluator.evaluate(model.transform(train_data)))\n      test_r2.append(evaluator.evaluate(model.transform(test_data)))\n      coef.append(model.bestModel.stages[-1].coefficients.toArray())\n\nsearch_lambda_alpha(train_data, test_data,\n                    _elastic_net_lambda, _elastic_net_alpha,\n                    _elastic_net_train_r2, _elastic_net_test_r2, _elastic_net_coef)\n"],"metadata":{"colab":{},"colab_type":"code","id":"Vq1EnhRsCcJ8"},"outputs":[],"execution_count":143},{"cell_type":"code","source":["plt.clf()\nplt.figure(figsize=(20, 17))\n\nplt.clf()\n\nfig, ax = plt.subplots(2, 1, figsize=(20, 7))\nfor idx, a in enumerate(_elastic_net_alpha):\n    coef_idx = len(_elastic_net_lambda) * idx\n    ax[0].plot(_elastic_net_lambda, _elastic_net_train_r2[coef_idx:coef_idx+len(_elastic_net_lambda)],\n               'o-', linewidth=2, label=f\"Training Set-Alpha: {a}\")\n    ax[0].plot(_elastic_net_lambda, _elastic_net_test_r2[coef_idx:coef_idx+len(_elastic_net_lambda)],\n               'o-', linewidth=2, label=f\"Test Set-Alpha: {a}\")\n    ax[0].set_title('Elastic Net Regression Performance', fontSize=25)\n    ax[0].set_xlabel('R^2')\n    ax[0].set_ylabel('lambda')\n    diff = [pair[0] - pair[1] for pair in zip(\n        _elastic_net_train_r2[coef_idx:coef_idx+len(_elastic_net_lambda)], \n        _elastic_net_test_r2[coef_idx:coef_idx+len(_elastic_net_lambda)])]\n    ax[1].plot(_elastic_net_lambda, diff, 'o-', linewidth=2, label=f\"Difference of R^2-Alpha: {a}\")\n    ax[1].set_title('Difference of the Elastic Net Regression Preference', fontSize=25)\n    ax[1].set_xlabel('lambda')\n    ax[1].set_ylabel('Difference of R^2')\n\n    ax[0].legend()\n    ax[1].legend()\n\n    plt.tight_layout()\n    display()\n"],"metadata":{"colab":{},"colab_type":"code","id":"GCVGWzDodQU_"},"outputs":[],"execution_count":144},{"cell_type":"markdown","source":["#### Feature Importance Visualization"],"metadata":{"colab_type":"text","id":"YcOKaUusitv6"}},{"cell_type":"code","source":["reg_coef = pd.DataFrame(_elastic_net_coef, columns=[col_name for col_name in column_selector.selected_columns if col_name not in ['price', 'id']])\n\nplt.clf()\n\nfig = plt.figure(figsize=(25, 10))\n\nsns.boxplot(data=reg_coef.abs(), orient='h')\n\nplt.title(f\"Distribution of Feature Regression Coefficient\", fontsize=30)\nplt.ylabel('Feature Name', fontsize=18)\nplt.yticks(fontsize=15)\nplt.xlabel('Regression Coefficient', fontsize=18)\n\nplt.tight_layout()\ndisplay()\n\nreturn"],"metadata":{"colab":{},"colab_type":"code","id":"ebIiV7-EjQZV"},"outputs":[],"execution_count":146},{"cell_type":"markdown","source":["#### Insights\n\n1. Rooms clearance indicates the class of a house. Airbnb can dispatch vouchers to cover miscellaneous fee, like cleanning fee to increase retension rate.\n2. A flexible arrangement is a big determinent. Guest may pay extra price to buy the offer such as booking travel protection."],"metadata":{"colab_type":"text","id":"9Hi_aoMcdSgY"}},{"cell_type":"markdown","source":["## Natrual Language Processing"],"metadata":{"colab_type":"text","id":"diAIU3o4hy3f"}},{"cell_type":"markdown","source":["### Term Frequency in Host Description"],"metadata":{"colab_type":"text","id":"zEjRZHiTdJbB"}},{"cell_type":"code","source":["nlp_df = load_data('listings.csv')\nnlp_df1=nlp_df.select(\"name\",\"review_scores_rating\",\"description\")"],"metadata":{"colab":{},"colab_type":"code","id":"h4-E_9n5iSlk"},"outputs":[],"execution_count":150},{"cell_type":"code","source":["nlp_df1.printSchema()\nnlp_df1.count(),len(nlp_df1.columns)"],"metadata":{"colab":{},"colab_type":"code","id":"lZR321XqlKLb"},"outputs":[],"execution_count":151},{"cell_type":"code","source":["from pyspark.sql.types import IntegerType\nnlp_df1 = nlp_df1.withColumn(\"review_scores_rating\", nlp_df1[\"review_scores_rating\"].cast(IntegerType()))"],"metadata":{},"outputs":[],"execution_count":152},{"cell_type":"code","source":["nlp_df2=nlp_df1.where(nlp_df1.review_scores_rating == 100)\nnlp_df2.printSchema()"],"metadata":{"colab":{},"colab_type":"code","id":"GTtLzQCJlS3p"},"outputs":[],"execution_count":153},{"cell_type":"code","source":["nlp_df2.count(),len(nlp_df2.columns)"],"metadata":{},"outputs":[],"execution_count":154},{"cell_type":"code","source":["from langdetect import detect\ndef language_detection(text):\n    try:\n        return detect(text)\n    except:\n        return None\nlanguage_udf = udf(language_detection)\nnlp_df2 = nlp_df2.withColumn('language',language_udf(nlp_df2['description']))\nnlp_df2= nlp_df2.filter(nlp_df2['language']=='en')"],"metadata":{},"outputs":[],"execution_count":155},{"cell_type":"code","source":["from pyspark.ml.feature import RegexTokenizer\ntokenizer = RegexTokenizer(minTokenLength=3).setGaps(False)\\\n  .setPattern(\"\\\\p{L}+\")\\\n  .setInputCol(\"description\")\\\n  .setOutputCol(\"words\")\nimport requests\nstop_words = requests.get('http://ir.dcs.gla.ac.uk/resources/linguistic_utils/stop_words').text.split()"],"metadata":{"colab":{},"colab_type":"code","id":"RYkxQJkPm2rG"},"outputs":[],"execution_count":156},{"cell_type":"code","source":["from pyspark.ml.feature import StopWordsRemover\nsw_filter = StopWordsRemover()\\\n  .setStopWords(stop_words)\\\n  .setCaseSensitive(False)\\\n  .setInputCol(\"words\")\\\n  .setOutputCol(\"filtered_d\")"],"metadata":{"colab":{},"colab_type":"code","id":"ajx2Na6xmUuL"},"outputs":[],"execution_count":157},{"cell_type":"code","source":["nlp_pipeline = Pipeline(stages=[tokenizer, sw_filter]).fit(nlp_df2)\nnlp_df3 = nlp_pipeline.transform(nlp_df2)"],"metadata":{"colab":{},"colab_type":"code","id":"e3v9kXN1nGlT"},"outputs":[],"execution_count":158},{"cell_type":"code","source":["from pyspark.ml.feature import CountVectorizer\ncv = CountVectorizer().setInputCol('filtered_d').setOutputCol(\"tf\")\nfrom pyspark.ml.feature import IDF\nidf = IDF().\\\n    setInputCol('tf').\\\n    setOutputCol('tfidf')"],"metadata":{"colab":{},"colab_type":"code","id":"c7qjHcLvnOiS"},"outputs":[],"execution_count":159},{"cell_type":"code","source":["tfidf_pipeline=Pipeline(stages=[cv,idf]).fit(nlp_df3)\nnlp_df4 = tfidf_pipeline.transform(nlp_df3)\n\nwords = tfidf_pipeline.stages[0].vocabulary\nIDF_values = tfidf_pipeline.stages[1].idf"],"metadata":{"colab":{},"colab_type":"code","id":"idM39SDKnQju"},"outputs":[],"execution_count":160},{"cell_type":"code","source":["voca_idf = pd.DataFrame({'word': words, 'IDF': IDF_values})\nhighest_idf=voca_idf.sort_values('IDF',ascending= False)"],"metadata":{"colab":{},"colab_type":"code","id":"FBSTHr9PnbaN"},"outputs":[],"execution_count":161},{"cell_type":"code","source":["highest_idf.head(20)"],"metadata":{"colab":{},"colab_type":"code","id":"sMvfhS9dnesJ"},"outputs":[],"execution_count":162},{"cell_type":"code","source":["from wordcloud import WordCloud\nwordcloudConvertDF =voca_idf.set_index('word').T.to_dict('records')\nwordcloud = WordCloud(width=800, height=500, random_state=21, max_font_size=100, relative_scaling=0.5, colormap='Dark2').generate_from_frequencies(dict(*wordcloudConvertDF))\nplt.figure(figsize=(14, 10))    \nplt.imshow(wordcloud, interpolation=\"bilinear\")\nplt.axis('off')\ndisplay(plt.show())"],"metadata":{},"outputs":[],"execution_count":163},{"cell_type":"markdown","source":["### Comments sentiment ananlysis"],"metadata":{"colab_type":"text","id":"a0DPwYtCofng"}},{"cell_type":"code","source":["review_df=df_reviews\nreview_df.toPandas().head()\nreview_df = review_df.dropna()"],"metadata":{"colab":{},"colab_type":"code","id":"6NFK7zVBoeEl"},"outputs":[],"execution_count":165},{"cell_type":"code","source":["#This cell we randomly extract 4500 comments as sample to analyze instead of whole dataset to decrease the time, we can get the whole data result without running this cell\nreview_df=review_df.sample(False, 0.1, seed=21).limit(5000)"],"metadata":{},"outputs":[],"execution_count":166},{"cell_type":"code","source":["import nltk\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer\nnltk.download('vader_lexicon')\nanalyzer = SentimentIntensityAnalyzer()\n\ndef print_sentiment_scores(sentence):\n    snt = analyzer.polarity_scores(sentence)\n    print(\"{:-<40} {}\".format(sentence, str(snt)))\ndef compound_score(text):\n    compound_value = analyzer.polarity_scores(text)['compound']\n   \n    return compound_value"],"metadata":{"colab":{},"colab_type":"code","id":"ze79xeQxplbk"},"outputs":[],"execution_count":167},{"cell_type":"code","source":["from pyspark.sql.functions import udf\nfrom pyspark.sql.types import FloatType\nscore_udf = udf(compound_score,FloatType())\nreview_df = review_df.withColumn('sentiment_compound',score_udf(review_df['comments']))"],"metadata":{"colab":{},"colab_type":"code","id":"KkLMTc0EqclQ"},"outputs":[],"execution_count":168},{"cell_type":"code","source":["from langdetect import detect\ndef language_detection(text):\n    try:\n        return detect(text)\n    except:\n        return None\n      \nlanguage_udf = udf(language_detection)\nreview_df = review_df.withColumn('language',language_udf(review_df['comments']))\nreview_df = review_df.filter(review_df['language']=='en')"],"metadata":{},"outputs":[],"execution_count":169},{"cell_type":"code","source":["from pyspark.ml.feature import RegexTokenizer\ntokenizer2 = RegexTokenizer(minTokenLength=3).setGaps(False)\\\n  .setPattern(\"\\\\p{L}+\")\\\n  .setInputCol(\"comments\")\\\n  .setOutputCol(\"words\")\nimport requests\nstop_words = requests.get('http://ir.dcs.gla.ac.uk/resources/linguistic_utils/stop_words').text.split()\nfrom pyspark.ml.feature import StopWordsRemover\nsw_filter2 = StopWordsRemover()\\\n  .setStopWords(stop_words)\\\n  .setCaseSensitive(False)\\\n  .setInputCol(\"words\")\\\n  .setOutputCol(\"filtered_com\")\nfrom pyspark.ml.feature import CountVectorizer\ncv2 = CountVectorizer().setInputCol('filtered_com').setOutputCol(\"tf\")\nfrom pyspark.ml.feature import IDF\nidf2 = IDF().\\\n    setInputCol('tf').\\\n    setOutputCol('tfidf')"],"metadata":{"colab":{},"colab_type":"code","id":"VjDfcuoXrzME"},"outputs":[],"execution_count":170},{"cell_type":"code","source":["nlp_pipeline2 = Pipeline(stages=[tokenizer2, sw_filter2, cv2, idf2]).fit(review_df)\nreview_df1 = nlp_pipeline2.transform(review_df).cache()"],"metadata":{"colab":{},"colab_type":"code","id":"deVg3Je-r9RC"},"outputs":[],"execution_count":171},{"cell_type":"code","source":["review_df2 = review_df1.select('listing_id','reviewer_id','filtered_com','comments','sentiment_compound','tf','tfidf')"],"metadata":{"colab":{},"colab_type":"code","id":"_61NLkgTsIc0"},"outputs":[],"execution_count":172},{"cell_type":"code","source":["review=review_df2.select(fn.when(fn.col('sentiment_compound') > 0,1).otherwise(0).alias(\"score\"),\n                                'listing_id','comments')"],"metadata":{"colab":{},"colab_type":"code","id":"EoMvhMD0sVlK"},"outputs":[],"execution_count":173},{"cell_type":"code","source":["review.groupby('score').count().show()"],"metadata":{"colab":{},"colab_type":"code","id":"CCpKdbQas7OI"},"outputs":[],"execution_count":174},{"cell_type":"code","source":["reviewp = review.filter(review['score']=='1')\nreviewn= review.filter(review['score']=='0')\nreviewp=reviewp.sample(False, 0.1, seed=0).limit(100)"],"metadata":{},"outputs":[],"execution_count":175},{"cell_type":"code","source":["reviewc = reviewp.union(reviewn)"],"metadata":{},"outputs":[],"execution_count":176},{"cell_type":"code","source":["from pyspark.ml.classification import LogisticRegression\ntraining_df, validation_df, testing_df = reviewc.randomSplit([0.5, 0.3, 0.2], seed=0)"],"metadata":{"colab":{},"colab_type":"code","id":"hvhSVya8uBzP"},"outputs":[],"execution_count":177},{"cell_type":"code","source":["lr = LogisticRegression().\\\n    setLabelCol('score').\\\n    setFeaturesCol('tfidf').\\\n    setRegParam(0.0).\\\n    setMaxIter(100).\\\n    setElasticNetParam(0.)\nlr_pipeline = Pipeline(stages=[nlp_pipeline2, lr]).fit(training_df)"],"metadata":{"colab":{},"colab_type":"code","id":"Ob6wTtghuGim"},"outputs":[],"execution_count":178},{"cell_type":"code","source":["lr_pipeline.transform(validation_df).\\\n    select(fn.expr('float(prediction = score)').alias('correct')).\\\n    select(fn.avg('correct')).show()"],"metadata":{"colab":{},"colab_type":"code","id":"tKnSoMeTvc7o"},"outputs":[],"execution_count":179},{"cell_type":"markdown","source":["### Sentiment Prediction Model tuning"],"metadata":{"colab_type":"text","id":"if1-opclwmxQ"}},{"cell_type":"code","source":["vocabulary = nlp_pipeline2.stages[2].vocabulary\nweights = lr_pipeline.stages[-1].coefficients.toArray()"],"metadata":{"colab":{},"colab_type":"code","id":"1fVR2LvyyO2P"},"outputs":[],"execution_count":181},{"cell_type":"code","source":["coeffs_df = pd.DataFrame({'word': vocabulary, 'weight': weights})\ncoeffs_df.head()"],"metadata":{"colab":{},"colab_type":"code","id":"0p5agCjWxahR"},"outputs":[],"execution_count":182},{"cell_type":"code","source":["coeffs_df.sort_values('weight').head(5)"],"metadata":{"colab":{},"colab_type":"code","id":"F6jN_e_5yVrz"},"outputs":[],"execution_count":183},{"cell_type":"code","source":["coeffs_df.sort_values('weight', ascending=False).head(5)"],"metadata":{"colab":{},"colab_type":"code","id":"cH1847VPyYW9"},"outputs":[],"execution_count":184},{"cell_type":"markdown","source":["It seems a lot of noise in this model and the model is overfiiting cuz these words don't make sense"],"metadata":{"colab_type":"text","id":"e5xFAMw6yeXa"}},{"cell_type":"code","source":["lambda_par = 0.02\nalpha_par = 0.3\nen_lr = LogisticRegression().\\\n        setLabelCol('score').\\\n        setFeaturesCol('tfidf').\\\n        setRegParam(lambda_par).\\\n        setMaxIter(100).\\\n        setElasticNetParam(alpha_par)"],"metadata":{"colab":{},"colab_type":"code","id":"8ziXtRvAybZC"},"outputs":[],"execution_count":186},{"cell_type":"code","source":["en_lr_estimator = Pipeline(\n    stages=[tokenizer2, sw_filter2, cv2, idf2, en_lr])"],"metadata":{"colab":{},"colab_type":"code","id":"hVejOzjDyyJI"},"outputs":[],"execution_count":187},{"cell_type":"code","source":["en_lr_pipeline = en_lr_estimator.fit(training_df)\nen_lr_pipeline.transform(validation_df).select(fn.avg(fn.expr('float(prediction = score)'))).show()"],"metadata":{"colab":{},"colab_type":"code","id":"MhCBdUETy5Hz"},"outputs":[],"execution_count":188},{"cell_type":"code","source":["en_weights = en_lr_pipeline.stages[-1].coefficients.toArray()\nen_coeffs_df = pd.DataFrame({'word': en_lr_pipeline.stages[2].vocabulary, 'weight': en_weights})"],"metadata":{"colab":{},"colab_type":"code","id":"oQ58sQ3I2o4N"},"outputs":[],"execution_count":189},{"cell_type":"code","source":["en_coeffs_df.sort_values('weight').head(15)"],"metadata":{"colab":{},"colab_type":"code","id":"5YZqUfrY2rWg"},"outputs":[],"execution_count":190},{"cell_type":"code","source":["en_coeffs_df.sort_values('weight', ascending=False).head(15)"],"metadata":{"colab":{},"colab_type":"code","id":"JxnwB5wU2uay"},"outputs":[],"execution_count":191},{"cell_type":"code","source":["en_coeffs_df.query('weight == 0.0').shape"],"metadata":{"colab":{},"colab_type":"code","id":"Oa0gKiOd20p1"},"outputs":[],"execution_count":192},{"cell_type":"code","source":["en_coeffs_df.query('weight == 0.0').shape[0]/en_coeffs_df.shape[0]"],"metadata":{"colab":{},"colab_type":"code","id":"YmKUlLyE249f"},"outputs":[],"execution_count":193},{"cell_type":"code","source":["en_coeffs_df.query('weight == 0.0').head(15)"],"metadata":{"colab":{},"colab_type":"code","id":"Ui59ff6S270o"},"outputs":[],"execution_count":194},{"cell_type":"code","source":["from pyspark.ml.tuning import ParamGridBuilder\ngrid = ParamGridBuilder().\\\n    addGrid(en_lr.regParam, [0., 0.01, 0.02]).\\\n    addGrid(en_lr.elasticNetParam, [0., 0.2, 0.4]).\\\n    build()"],"metadata":{"colab":{},"colab_type":"code","id":"8mdioI8z2-pW"},"outputs":[],"execution_count":195},{"cell_type":"code","source":["all_models = []\nfor j in range(len(grid)):\n    print(\"Fitting model {}\".format(j+1))\n    model = en_lr_estimator.fit(training_df, grid[j])\n    all_models.append(model)"],"metadata":{"colab":{},"colab_type":"code","id":"NvywRxQI3CUP"},"outputs":[],"execution_count":196},{"cell_type":"code","source":["accuracies = [m.\\\n    transform(validation_df).\\\n    select(fn.avg(fn.expr('float(score = prediction)')).alias('accuracy')).\\\n    first().\\\n    accuracy for m in all_models]"],"metadata":{"colab":{},"colab_type":"code","id":"UMKcYZlW3FaV"},"outputs":[],"execution_count":197},{"cell_type":"code","source":["best_model_idx = np.argmax(accuracies)\nprint(\"best model index =\", best_model_idx)"],"metadata":{"colab":{},"colab_type":"code","id":"9CmbNtCn3I5e"},"outputs":[],"execution_count":198},{"cell_type":"code","source":["grid[best_model_idx]"],"metadata":{"colab":{},"colab_type":"code","id":"5IH8CvSN3LqC"},"outputs":[],"execution_count":199},{"cell_type":"code","source":["best_model = all_models[best_model_idx]"],"metadata":{"colab":{},"colab_type":"code","id":"tqyeS76I3QgO"},"outputs":[],"execution_count":200},{"cell_type":"code","source":["best_model.\\\n    transform(testing_df).\\\n    select(fn.avg(fn.expr('float(score = prediction)')).alias('accuracy')).\\\n    show()"],"metadata":{"colab":{},"colab_type":"code","id":"mvsvqyrU3R7k"},"outputs":[],"execution_count":201},{"cell_type":"code","source":["most_living.show()"],"metadata":{},"outputs":[],"execution_count":202},{"cell_type":"markdown","source":["## Recommend System"],"metadata":{"colab_type":"text","id":"FbbOweilUx6T"}},{"cell_type":"markdown","source":["### Based on Users (The score of reviewers)"],"metadata":{}},{"cell_type":"markdown","source":["This system used collaborative filtering to help users get which houses they could live according to their past reviews. Spark supports Alternating Least Square(ALS) to factor decomposition which that's only one spark supports, because the purpose of spark is generally for big data, ALS doesn't need the algorithm to calculator all the results, so it would save time and computer calculation."],"metadata":{}},{"cell_type":"code","source":["from pyspark.ml.recommendation import ALS\nfrom pyspark.ml.evaluation import RegressionEvaluator\n(training, test) = review_df1.randomSplit([0.7, 0.3])\nals = ALS(maxIter=5, regParam=0.01, userCol=\"reviewer_id\", itemCol=\"listing_id\", ratingCol=\"sentiment_compound\",\n          coldStartStrategy=\"drop\")\nmodel = als.fit(training)\npredictions = model.transform(test)\nevaluator = RegressionEvaluator(metricName=\"rmse\", labelCol=\"sentiment_compound\",\n                                predictionCol=\"prediction\")\nrmse = evaluator.evaluate(predictions)\nprint(\"Root-mean-square error = \" + str(rmse))\n\n# Generate top 10 house recommendations for each user\nuserRecs = model.recommendForAllUsers(10)\n# Generate top 10 user recommendations for each house\nhouseRecs = model.recommendForAllItems(10)"],"metadata":{"colab":{},"colab_type":"code","id":"DFIf4wdcUzZ7"},"outputs":[],"execution_count":206},{"cell_type":"code","source":["userRecs.show()"],"metadata":{"colab":{},"colab_type":"code","id":"gvk3epl5cAVa"},"outputs":[],"execution_count":207},{"cell_type":"code","source":["houseRecs.show()"],"metadata":{"colab":{},"colab_type":"code","id":"ZjBlZdJ-dqF6"},"outputs":[],"execution_count":208},{"cell_type":"markdown","source":["### Based on House (Description)"],"metadata":{}},{"cell_type":"markdown","source":["Created the data for this system which is based on the description of houses."],"metadata":{}},{"cell_type":"code","source":["df3 = load_data('listings.csv').select(['id','description'])\nrec_most_living = most_living.drop('listing_id').join(df3,\"id\", \"left_outer\").limit(50)"],"metadata":{},"outputs":[],"execution_count":211},{"cell_type":"markdown","source":["Built pipeline for model."],"metadata":{}},{"cell_type":"code","source":["from pyspark.ml import clustering\nfrom pyspark.ml.feature import StandardScaler\nfrom pyspark.ml.feature import Normalizer\nfrom pyspark.ml.feature import PCA\n\ncenter = StandardScaler(withMean=True, withStd=False, inputCol='tfidf', outputCol='centered_tfidf')\nnorm = Normalizer(inputCol=\"centered_tfidf\", outputCol=\"norm_tfidf\", p=2.0)\nkmeans = clustering.KMeans(k=10, featuresCol='norm_tfidf', predictionCol='kmeans_feat')\npca = PCA(k=10, inputCol='centered_tfidf', outputCol='scores')\nrec_pipeline = Pipeline(stages=[tokenizer, sw_filter, cv, idf, center, norm, kmeans, pca])\npipeline_model = rec_pipeline.fit(rec_most_living)     "],"metadata":{},"outputs":[],"execution_count":213},{"cell_type":"markdown","source":["Define a User Defined Function (UDF) that takes as input two column vectors and returns the distance between them."],"metadata":{}},{"cell_type":"code","source":["from pyspark.sql import types\n\ndef l2_dist(c1, c2):    \n    return float(np.sqrt((c1 - c2).T.dot((c1 - c2))))\n\nl2_dist_udf = fn.udf(l2_dist, types.FloatType())"],"metadata":{},"outputs":[],"execution_count":215},{"cell_type":"markdown","source":["Built a function"],"metadata":{}},{"cell_type":"code","source":["def recommemd(id,num):\n  print(f'There are {num} houses in Airbnb you could like except {id}')\n\n  result = pipeline_model.transform(rec_most_living).\\\n        where(rec_most_living.id == id).\\\n        select(fn.col(\"scores\").alias('rec_scores')).\\\n        join(pipeline_model.transform(rec_most_living)).\\\n        withColumn('distance', l2_dist_udf('scores', 'rec_scores')).\\\n        select(\"id\",\"host_name\", \"listing_url\",\"description\", \"distance\").\\\n        orderBy(fn.asc(\"distance\")).\\\n        limit(num+1).show()\n  return result"],"metadata":{},"outputs":[],"execution_count":217},{"cell_type":"markdown","source":["Test this recommender system"],"metadata":{}},{"cell_type":"code","source":["recommemd(\"5259194\",5)"],"metadata":{},"outputs":[],"execution_count":219},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":220},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":221}],"metadata":{"language_info":{"mimetype":"text/x-python","name":"python","pygments_lexer":"ipython3","codemirror_mode":{"name":"ipython","version":3},"version":"3.7.4","nbconvert_exporter":"python","file_extension":".py"},"name":"718_project","notebookId":2268284004316058,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"colab":{"collapsed_sections":["OeH6aPY8BT4K"],"name":"718_project","provenance":[],"toc_visible":true},"toc":{"title_sidebar":"Contents","nav_menu":{},"sideBar":true,"number_sections":true,"skip_h1_title":false,"base_numbering":1,"toc_cell":false,"toc_position":{},"toc_section_display":true,"toc_window_display":false,"title_cell":"Table of Contents"}},"nbformat":4,"nbformat_minor":0}
